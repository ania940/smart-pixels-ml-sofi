{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24773cdc-4bbe-48c3-9910-8b39c38bfc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 21:41:27.814389: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-16 21:41:27.814454: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-16 21:41:27.815315: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-16 21:41:27.825884: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-16 21:41:28.845798: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#This notebook was used to train a transformer model with sum of stds reg loss (0.5 of weight)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from qkeras import *\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from loss import custom_loss\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "pi = 3.14159265359\n",
    "\n",
    "maxval=1e9\n",
    "minval=1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff499d81-383f-4988-94d7-12815179e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac065e8-fcf2-44a0-8dee-e1e44e605137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 21:41:32.215899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38397 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB MIG 7g.40gb, pci bus id: 0000:81:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "#from dataprep import *\n",
    "from OptimizedDataGenerator import OptimizedDataGenerator\n",
    "import tensorflow_probability as tfp\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e1a0184-55a3-4294-b0df-b3067d9bb72f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Quantization is True in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevo directorio para train: /home/callea/TFrecords/TFR_train\n",
      "Nuevo directorio para validation: /home/callea/TFrecords/TFR_val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Quantization is True in data generator. This may affect model performance.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5000\n",
    "val_batch_size = 5000\n",
    "train_file_size = 50  # Make this larger (150?)\n",
    "val_file_size = 10\n",
    "\n",
    "dataset_base_dir = f'/depot/cms/users/das214/dataset8/unflipped/'\n",
    "tfrecords_base_dir = os.path.expanduser(\"~/TFrecords/\")\n",
    "os.makedirs(tfrecords_base_dir, exist_ok=True)\n",
    "\n",
    "data_directory_path = os.path.join(dataset_base_dir, 'recon3D/')\n",
    "labels_directory_path = os.path.join(dataset_base_dir, 'labels/')\n",
    "\n",
    "tfrecords_dir_train = os.path.join(tfrecords_base_dir, 'TFR_train')\n",
    "tfrecords_dir_validation = os.path.join(tfrecords_base_dir, 'TFR_val')\n",
    "\n",
    "print(\"Nuevo directorio para train:\", tfrecords_dir_train)\n",
    "print(\"Nuevo directorio para validation:\", tfrecords_dir_validation)\n",
    "\n",
    "# validation_generator = OptimizedDataGenerator(\n",
    "#      data_directory_path = data_directory_path,\n",
    "#      labels_directory_path = labels_directory_path,\n",
    "#      is_directory_recursive = False,\n",
    "#      file_type = \"parquet\",\n",
    "#      data_format = \"3D\",\n",
    "#      batch_size = val_batch_size,\n",
    "#      file_count = val_file_size,\n",
    "#      to_standardize= True,\n",
    "#      include_y_local= False, \n",
    "#      labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "#      input_shape = (2,13,21), # (20,13,21),\n",
    "#      transpose = (0,2,3,1),\n",
    "#      shuffle = False, \n",
    "#      files_from_end=True,\n",
    "#      tfrecords_dir = tfrecords_dir_validation,\n",
    "#      use_time_stamps = [0, 19], #-1\n",
    "#      max_workers = 1 # Don't make this too large (will use up all RAM)\n",
    "#  )\n",
    "\n",
    "# training_generator = OptimizedDataGenerator(\n",
    "#      data_directory_path = data_directory_path,\n",
    "#      labels_directory_path = labels_directory_path,\n",
    "#      is_directory_recursive = False,\n",
    "#      file_type = \"parquet\",\n",
    "#      data_format = \"3D\",\n",
    "#      batch_size = batch_size,\n",
    "#      file_count = train_file_size,\n",
    "#      to_standardize= True,\n",
    "#      include_y_local= False,\n",
    "#      labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "#      input_shape = (2,13,21), # (20,13,21),\n",
    "#      transpose = (0,2,3,1),\n",
    "#      shuffle = False, # True \n",
    "\n",
    "#      tfrecords_dir = tfrecords_dir_train,\n",
    "#      use_time_stamps = [0, 19], #-1\n",
    "#      max_workers = 1 # Don't make this too large (will use up all RAM)\n",
    "#  )\n",
    "\n",
    "training_generator = OptimizedDataGenerator(\n",
    "    load_from_tfrecords_dir = tfrecords_dir_train,\n",
    "    shuffle = True,\n",
    "    seed = 13,\n",
    "    quantize = True\n",
    ")\n",
    "\n",
    "validation_generator = OptimizedDataGenerator(\n",
    "    load_from_tfrecords_dir = tfrecords_dir_validation,\n",
    "    shuffle = True,\n",
    "    seed = 13,\n",
    "    quantize = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3013a7-2aa9-413c-9a0a-919090c61b5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(data_directory_path)))\n",
    "print(len(os.listdir(labels_directory_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "632335b8-818e-41cb-98e0-b6dc10f61957",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1 = os.listdir(data_directory_path)[0]\n",
    "path_1 = os.path.join(data_directory_path, path_1)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df1 = pd.read_parquet(path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94de66e5-dc94-441e-8947-261b67184c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df1.iloc[0]\n",
    "\n",
    "data_files_path_list = os.listdir(data_directory_path)\n",
    "labels_files_path_list = os.listdir(labels_directory_path)\n",
    "\n",
    "data_df = pd.read_parquet(os.path.join(data_directory_path, data_files_path_list[0])) \n",
    "labels_df = pd.read_parquet(os.path.join(labels_directory_path, labels_files_path_list[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "015749bd-de47-491a-98bc-732748c423b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b91f4666-1358-4cff-8ff0-32be68a6b28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAFeCAYAAABXfh8zAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHQdJREFUeJzt3XtwVPX9//HXkssGmGQRNJeVANGhIJeiBpCbglKDERDHVkAtxlt/WlFEnArUWtGOBq2lTKVgcRBxLMi0XMqvWDFqAlhAAwlK0XKRVDKFNNWf7gYoS0g+vz++X3YM2YQsPYfP7vJ8zJwZ95zP53Pen/l43Jdnd3M8xhgjAAAAS9rZLgAAAJzfCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsCrZdgGna2xs1KFDh5Seni6Px2O7HAAA0AbGGNXV1cnv96tdu+judcRcGDl06JByc3NtlwEAAM5CdXW1unbtGlWfmAsj6enpkqT80X9QUnIHy9UAAIC2aDh5TDveuzX8Ph6NmAsjpz6aSUruoOSUjparAQAA0Tibr1jwBVYAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYJVrYWThwoXKy8tTWlqa8vPztXnzZrdOBQAA4pgrYWTlypWaPn26nnjiCVVWVurqq69WYWGhDh486MbpAABAHHMljMybN0/33nuv7rvvPl122WWaP3++cnNztWjRIjdOBwAA4pjjYeTEiRPasWOHCgoKmuwvKCjQli1bmrUPhUIKBoNNNgAAcP5wPIx8+eWXamhoUFZWVpP9WVlZqqmpada+uLhYPp8vvPFcGgAAzi+ufYH19D8Ha4yJ+CdiZ8+erUAgEN6qq6vdKgkAAMQgx59Nc+GFFyopKanZXZDa2tpmd0skyev1yuv1Ol0GAACIE47fGUlNTVV+fr5KSkqa7C8pKdGwYcOcPh0AAIhzrjy1d8aMGZoyZYoGDhyooUOHavHixTp48KAeeOABN04HAADimCthZNKkSfrqq6/0zDPP6PDhw+rXr5/eeustde/e3Y3TAQCAOOYxxhjbRXxbMBiUz+fT4DHrlZzS0XY5AACgDU7WH9VHG8YqEAgoIyMjqr48mwYAAFhFGAEAAFYRRgAAgFWEEQAAYJUrv6YBAKA1Kd5U18auD51wbWy4gzsjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsCrZdgEAkKi6XJzl6vizpl3s2tiX/3u9a2NLUmjrZtfGHvfXia6NDXdwZwQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABY5XgYKS4u1qBBg5Senq7MzEzdfPPN2rNnj9OnAQAACcLxMLJx40ZNnTpV27ZtU0lJiU6ePKmCggIdPXrU6VMBAIAE4Pizad5+++0mr5cuXarMzEzt2LFD11xzTbP2oVBIoVAo/DoYDDpdEgAAiGGuf2ckEAhIkjp37hzxeHFxsXw+X3jLzc11uyQAABBDXA0jxhjNmDFDI0aMUL9+/SK2mT17tgKBQHirrq52syQAABBjHP+Y5tseeughffLJJ/rggw9abOP1euX1et0sAwAAxDDXwsjDDz+sdevWadOmTeratatbpwEAAHHO8TBijNHDDz+sNWvWqKysTHl5eU6fAgAAJBDHw8jUqVO1fPly/elPf1J6erpqamokST6fT+3bt3f6dAAAIM45/gXWRYsWKRAIaNSoUcrJyQlvK1eudPpUAAAgAbjyMQ0AAEBb8WwaAABgFWEEAABYRRgBAABWufpHzxBfeg3u4+r4ez761NXx0bIuF2e5NvaEW3u5NrYkDez2b9fGPtno7n8C933ZydXxX/+/37g29uwP3X00x8n6ia6Oj/jCnREAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWJdsuALHjt0PWuzr+uE/7ujr+63O8ro19LDnDtbElKSP0b1fH393Q3bWxX11xyLWxJenVXftdHR+AfdwZAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFjlehgpLi6Wx+PR9OnT3T4VAACIQ66GkfLyci1evFjf/e533TwNAACIY66FkSNHjuiOO+7QK6+8ogsuuMCt0wAAgDjnWhiZOnWqxo4dq+9973uttguFQgoGg002AABw/nDlQXlvvvmmKioqVF5efsa2xcXFevrpp90oAwAAxAHH74xUV1frkUce0RtvvKG0tLQztp89e7YCgUB4q66udrokAAAQwxy/M7Jjxw7V1tYqPz8/vK+hoUGbNm3SggULFAqFlJSUFD7m9Xrl9br36HcAABDbHA8jo0eP1q5du5rsu/vuu9W7d2/NnDmzSRABAABwPIykp6erX79+TfZ17NhRXbp0abYfAACAv8AKAACscuXXNKcrKys7F6cBAABxiDsjAADAKsIIAACwijACAACsOiffGTmfLJ3fw9XxP/1/F7s29ntD/49rY0vS49s/cXX83cfdG/vpn21zb/Bz4iPbBQBAi7gzAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCqPMcbYLuLbgsGgfD6fBo9Zr+SUjrbLAQAAbXCy/qg+2jBWgUBAGRkZUfXlzggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKlfCyD//+U/98Ic/VJcuXdShQwddfvnl2rFjhxunAgAAcS7Z6QG//vprDR8+XNdee63+8pe/KDMzU59//rk6derk9KkAAEACcDyMPP/888rNzdXSpUvD+3r06OH0aQAAQIJw/GOadevWaeDAgbr11luVmZmpK664Qq+88kqL7UOhkILBYJMNAACcPxwPIwcOHNCiRYvUs2dPbdiwQQ888ICmTZum119/PWL74uJi+Xy+8Jabm+t0SQAAIIY5/tTe1NRUDRw4UFu2bAnvmzZtmsrLy7V169Zm7UOhkEKhUPh1MBhUbm4uT+0FACCOxNRTe3NyctSnT58m+y677DIdPHgwYnuv16uMjIwmGwAAOH84HkaGDx+uPXv2NNm3d+9ede/e3elTAQCABOB4GHn00Ue1bds2Pffcc9q/f7+WL1+uxYsXa+rUqU6fCgAAJADHw8igQYO0Zs0arVixQv369dMvfvELzZ8/X3fccYfTpwIAAAnA8b8zIknjxo3TuHHj3BgaAAAkGJ5NAwAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsMrxMHLy5En97Gc/U15entq3b69LLrlEzzzzjBobG50+FQAASADJTg/4/PPP6+WXX9ayZcvUt29fbd++XXfffbd8Pp8eeeQRp08HAADinONhZOvWrZowYYLGjh0rSerRo4dWrFih7du3O30qAACQABz/mGbEiBF67733tHfvXknSxx9/rA8++EA33nhjxPahUEjBYLDJBgAAzh+O3xmZOXOmAoGAevfuraSkJDU0NOjZZ5/VbbfdFrF9cXGxnn76aafLAAAAccLxOyMrV67UG2+8oeXLl6uiokLLli3Tiy++qGXLlkVsP3v2bAUCgfBWXV3tdEkAACCGOX5n5Cc/+YlmzZqlyZMnS5L69++vL774QsXFxSoqKmrW3uv1yuv1Ol0GAACIE47fGTl27JjatWs6bFJSEj/tBQAAETl+Z2T8+PF69tln1a1bN/Xt21eVlZWaN2+e7rnnHqdPBQAAEoDjYeSll17Sk08+qQcffFC1tbXy+/26//779fOf/9zpUwEAgATgMcYY20V8WzAYlM/n0+Ax65Wc0tF2OQAAoA1O1h/VRxvGKhAIKCMjI6q+PJsGAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYFXUY2bRpk8aPHy+/3y+Px6O1a9c2OW6M0Zw5c+T3+9W+fXuNGjVKu3fvdqpeAACQYKIOI0ePHtWAAQO0YMGCiMdfeOEFzZs3TwsWLFB5ebmys7N1/fXXq66u7r8uFgAAJJ7kaDsUFhaqsLAw4jFjjObPn68nnnhCt9xyiyRp2bJlysrK0vLly3X//fc36xMKhRQKhcKvg8FgtCUBAIA45uh3RqqqqlRTU6OCgoLwPq/Xq5EjR2rLli0R+xQXF8vn84W33NxcJ0sCAAAxztEwUlNTI0nKyspqsj8rKyt87HSzZ89WIBAIb9XV1U6WBAAAYlzUH9O0hcfjafLaGNNs3yler1der9eNMgAAQBxw9M5Idna2JDW7C1JbW9vsbgkAAIDkcBjJy8tTdna2SkpKwvtOnDihjRs3atiwYU6eCgAAJIioP6Y5cuSI9u/fH35dVVWlnTt3qnPnzurWrZumT5+u5557Tj179lTPnj313HPPqUOHDrr99tsdLRwAACSGqMPI9u3bde2114Zfz5gxQ5JUVFSk1157TY8//rj+85//6MEHH9TXX3+tq666Su+8847S09OdqxoAACQMjzHG2C7i24LBoHw+nwaPWa/klI62ywEAAG1wsv6oPtowVoFAQBkZGVH15dk0AADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAq6IOI5s2bdL48ePl9/vl8Xi0du3a8LH6+nrNnDlT/fv3V8eOHeX3+3XnnXfq0KFDTtYMAAASSNRh5OjRoxowYIAWLFjQ7NixY8dUUVGhJ598UhUVFVq9erX27t2rm266yZFiAQBA4kmOtkNhYaEKCwsjHvP5fCopKWmy76WXXtLgwYN18OBBdevWrVmfUCikUCgUfh0MBqMtCQAAxDHXvzMSCATk8XjUqVOniMeLi4vl8/nCW25urtslAQCAGOJqGDl+/LhmzZql22+/XRkZGRHbzJ49W4FAILxVV1e7WRIAAIgxUX9M01b19fWaPHmyGhsbtXDhwhbbeb1eeb1et8oAAAAxzpUwUl9fr4kTJ6qqqkrvv/9+i3dFAAAAHA8jp4LIvn37VFpaqi5dujh9CgAAkECiDiNHjhzR/v37w6+rqqq0c+dOde7cWX6/Xz/4wQ9UUVGhP//5z2poaFBNTY0kqXPnzkpNTXWucgAAkBCiDiPbt2/XtddeG349Y8YMSVJRUZHmzJmjdevWSZIuv/zyJv1KS0s1atSos68UAAAkpKjDyKhRo2SMafF4a8cAAABOx7NpAACAVYQRAABgFWEEAABY5dofPTtbp75z0nDymOVKAABAW5163z6b747GXBipq6uTJO1471bLlQAAgGjV1dXJ5/NF1cdjYuznL42NjTp06JDS09Pl8XjO2D4YDCo3N1fV1dUJ/5demWtiYq6J63yaL3NNTNHM1Rijuro6+f1+tWsX3bdAYu7OSLt27dS1a9eo+2VkZCT8vxSnMNfExFwT1/k0X+aamNo612jviJzCF1gBAIBVhBEAAGBV3IcRr9erp556Sl6v13YprmOuiYm5Jq7zab7MNTGdq7nG3BdYAQDA+SXu74wAAID4RhgBAABWEUYAAIBVhBEAAGAVYQQAAFgVF2Fk4cKFysvLU1pamvLz87V58+ZW22/cuFH5+flKS0vTJZdcopdffvkcVXr2iouLNWjQIKWnpyszM1M333yz9uzZ02qfsrIyeTyeZtvf//73c1T12ZkzZ06zmrOzs1vtE49rKkk9evSIuEZTp06N2D7e1nTTpk0aP368/H6/PB6P1q5d2+S4MUZz5syR3+9X+/btNWrUKO3evfuM465atUp9+vSR1+tVnz59tGbNGpdm0HatzbW+vl4zZ85U//791bFjR/n9ft155506dOhQq2O+9tprEdf7+PHjLs+mdWda17vuuqtZzUOGDDnjuPG2rpIiro/H49Evf/nLFseMxXVty3uMzes15sPIypUrNX36dD3xxBOqrKzU1VdfrcLCQh08eDBi+6qqKt144426+uqrVVlZqZ/+9KeaNm2aVq1adY4rj87GjRs1depUbdu2TSUlJTp58qQKCgp09OjRM/bds2ePDh8+HN569ux5Dir+7/Tt27dJzbt27WqxbbyuqSSVl5c3mWdJSYkk6dZbW38QZLys6dGjRzVgwAAtWLAg4vEXXnhB8+bN04IFC1ReXq7s7Gxdf/314QdiRrJ161ZNmjRJU6ZM0ccff6wpU6Zo4sSJ+vDDD92aRpu0Ntdjx46poqJCTz75pCoqKrR69Wrt3btXN9100xnHzcjIaLLWhw8fVlpamhtTaLMzrask3XDDDU1qfuutt1odMx7XVVKztXn11Vfl8Xj0/e9/v9VxY21d2/IeY/V6NTFu8ODB5oEHHmiyr3fv3mbWrFkR2z/++OOmd+/eTfbdf//9ZsiQIa7V6Iba2lojyWzcuLHFNqWlpUaS+frrr89dYQ546qmnzIABA9rcPlHW1BhjHnnkEXPppZeaxsbGiMfjdU2NMUaSWbNmTfh1Y2Ojyc7ONnPnzg3vO378uPH5fObll19ucZyJEyeaG264ocm+MWPGmMmTJzte89k6fa6RfPTRR0aS+eKLL1pss3TpUuPz+ZwtzmGR5lpUVGQmTJgQ1TiJsq4TJkww1113Xatt4mFdT3+PsX29xvSdkRMnTmjHjh0qKChosr+goEBbtmyJ2Gfr1q3N2o8ZM0bbt29XfX29a7U6LRAISJI6d+58xrZXXHGFcnJyNHr0aJWWlrpdmiP27dsnv9+vvLw8TZ48WQcOHGixbaKs6YkTJ/TGG2/onnvuOeMTqeNxTU9XVVWlmpqaJmvn9Xo1cuTIFq9fqeX1bq1PLAoEAvJ4POrUqVOr7Y4cOaLu3bura9euGjdunCorK89Ngf+lsrIyZWZm6jvf+Y5+9KMfqba2ttX2ibCu//rXv7R+/Xrde++9Z2wb6+t6+nuM7es1psPIl19+qYaGBmVlZTXZn5WVpZqamoh9ampqIrY/efKkvvzyS9dqdZIxRjNmzNCIESPUr1+/Ftvl5ORo8eLFWrVqlVavXq1evXpp9OjR2rRp0zmsNnpXXXWVXn/9dW3YsEGvvPKKampqNGzYMH311VcR2yfCmkrS2rVr9c033+iuu+5qsU28rmkkp67RaK7fU/2i7RNrjh8/rlmzZun2229v9UmnvXv31muvvaZ169ZpxYoVSktL0/Dhw7Vv375zWG30CgsL9fvf/17vv/++fvWrX6m8vFzXXXedQqFQi30SYV2XLVum9PR03XLLLa22i/V1jfQeY/t6TY6qtSWn/1+kMabV/7OM1D7S/lj10EMP6ZNPPtEHH3zQartevXqpV69e4ddDhw5VdXW1XnzxRV1zzTVul3nWCgsLw//cv39/DR06VJdeeqmWLVumGTNmROwT72sqSUuWLFFhYaH8fn+LbeJ1TVsT7fV7tn1iRX19vSZPnqzGxkYtXLiw1bZDhgxp8sXP4cOH68orr9RLL72k3/zmN26XetYmTZoU/ud+/fpp4MCB6t69u9avX9/qG3U8r6skvfrqq7rjjjvO+N2PWF/X1t5jbF2vMX1n5MILL1RSUlKzhFVbW9ssiZ2SnZ0dsX1ycrK6dOniWq1Oefjhh7Vu3TqVlpaqa9euUfcfMmRIzKTvturYsaP69+/fYt3xvqaS9MUXX+jdd9/VfffdF3XfeFxTSeFfSEVz/Z7qF22fWFFfX6+JEyeqqqpKJSUlrd4ViaRdu3YaNGhQ3K13Tk6Ounfv3mrd8byukrR582bt2bPnrK7hWFrXlt5jbF+vMR1GUlNTlZ+fH/4FwiklJSUaNmxYxD5Dhw5t1v6dd97RwIEDlZKS4lqt/y1jjB566CGtXr1a77//vvLy8s5qnMrKSuXk5DhcnbtCoZA+++yzFuuO1zX9tqVLlyozM1Njx46Num88rqkk5eXlKTs7u8nanThxQhs3bmzx+pVaXu/W+sSCU0Fk3759evfdd88qKBtjtHPnzrhb76+++krV1dWt1h2v63rKkiVLlJ+frwEDBkTdNxbW9UzvMdav16i+7mrBm2++aVJSUsySJUvMp59+aqZPn246duxo/vGPfxhjjJk1a5aZMmVKuP2BAwdMhw4dzKOPPmo+/fRTs2TJEpOSkmL++Mc/2ppCm/z4xz82Pp/PlJWVmcOHD4e3Y8eOhducPtdf//rXZs2aNWbv3r3mb3/7m5k1a5aRZFatWmVjCm322GOPmbKyMnPgwAGzbds2M27cOJOenp5wa3pKQ0OD6datm5k5c2azY/G+pnV1daaystJUVlYaSWbevHmmsrIy/AuSuXPnGp/PZ1avXm127dplbrvtNpOTk2OCwWB4jClTpjT5ddxf//pXk5SUZObOnWs+++wzM3fuXJOcnGy2bdt2zuf3ba3Ntb6+3tx0002ma9euZufOnU2u4VAoFB7j9LnOmTPHvP322+bzzz83lZWV5u677zbJycnmww8/tDHFsNbmWldXZx577DGzZcsWU1VVZUpLS83QoUPNxRdfnHDrekogEDAdOnQwixYtijhGPKxrW95jbF6vMR9GjDHmt7/9renevbtJTU01V155ZZOfuxYVFZmRI0c2aV9WVmauuOIKk5qaanr06NHiv0CxRFLEbenSpeE2p8/1+eefN5deeqlJS0szF1xwgRkxYoRZv379uS8+SpMmTTI5OTkmJSXF+P1+c8stt5jdu3eHjyfKmp6yYcMGI8ns2bOn2bF4X9NTP0U+fSsqKjLG/M/PBZ966imTnZ1tvF6vueaaa8yuXbuajDFy5Mhw+1P+8Ic/mF69epmUlBTTu3fvmAhjrc21qqqqxWu4tLQ0PMbpc50+fbrp1q2bSU1NNRdddJEpKCgwW7ZsOfeTO01rcz127JgpKCgwF110kUlJSTHdunUzRUVF5uDBg03GSIR1PeV3v/udad++vfnmm28ijhEP69qW9xib16vnf4sEAACwIqa/MwIAABIfYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABW/X8Wbs7z1DAvOQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reshaped_data = x.values.reshape(20, 13, 21)\n",
    "\n",
    "plt.imshow(reshaped_data[0,:,:], cmap='coolwarm') # first time-step\n",
    "plt.show()\n",
    "plt.imshow(reshaped_data[-1,:,:], cmap='coolwarm') # Last time-step\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a62d241-3682-4545-9e36-fec9d085567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3600e+01, -1.0760e+02,\n",
       "        -1.7900e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -7.0800e+01,  1.3220e+02,\n",
       "        -1.0530e+02,  5.0000e-01,  1.2500e+01,  8.3400e+01,  2.1300e+02,\n",
       "         2.9090e+02,  2.9870e+02,  2.4600e+02,  1.7910e+02,  1.7090e+02,\n",
       "         1.7680e+02,  1.2640e+02,  5.3400e+01,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00],\n",
       "       [-2.0000e-01, -6.1800e+01, -4.1700e+02, -6.6970e+02,  3.6834e+03,\n",
       "         1.1290e+02,  5.7400e+02,  2.5820e+02,  2.4320e+02,  2.0750e+02,\n",
       "         1.6760e+02,  1.5660e+02,  1.4910e+02,  1.4310e+02,  1.6630e+02,\n",
       "         1.7730e+02,  1.2670e+02,  5.3500e+01,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00],\n",
       "       [-2.0000e-01, -1.0820e+02,  3.2505e+03,  2.1142e+03,  5.1706e+03,\n",
       "         8.1400e+02,  8.9680e+02,  3.0440e+02,  2.5070e+02,  2.0620e+02,\n",
       "         1.6490e+02,  1.5410e+02,  1.4720e+02,  1.4200e+02,  1.6540e+02,\n",
       "         1.7670e+02,  1.2640e+02,  5.3300e+01,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00],\n",
       "       [-2.0000e-01, -1.0290e+02, -8.0380e+02, -7.5670e+02, -7.8410e+02,\n",
       "        -6.8300e+01,  2.5280e+02,  1.2020e+02,  8.3100e+01, -4.7600e+01,\n",
       "        -1.4270e+02, -1.5140e+02, -1.0150e+02, -3.7900e+01, -5.7000e+00,\n",
       "        -2.0000e-01, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00],\n",
       "       [-0.0000e+00, -2.0000e+00, -4.7000e+00, -6.0000e-01,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00],\n",
       "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_data[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9fd3960-c7f1-4d6c-b7e0-0c4c833797d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 13, 21, 2)]          0         []                            \n",
      "                                                                                                  \n",
      " patch_extractor (PatchExtr  (None, None, 42)             0         ['input_1[0][0]']             \n",
      " actor)                                                                                           \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncode  (None, 12, 64)               3520      ['patch_extractor[0][0]']     \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 12, 64)               128       ['patch_encoder[0][0]']       \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 12, 64)               66368     ['layer_normalization[0][0]', \n",
      " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 12, 64)               0         ['multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 12, 64)               0         ['dropout[0][0]',             \n",
      " Lambda)                                                             'patch_encoder[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 12, 64)               128       ['tf.__operators__.add[0][0]']\n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 12, 128)              8320      ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 12, 128)              0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 12, 64)               8256      ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 12, 64)               0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 12, 64)               0         ['dropout_2[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 12, 64)               128       ['tf.__operators__.add_1[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 12, 64)               66368     ['layer_normalization_2[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 12, 64)               0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 12, 64)               0         ['dropout_3[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 12, 64)               128       ['tf.__operators__.add_2[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 12, 128)              8320      ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 12, 128)              0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 12, 64)               8256      ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 12, 64)               0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, 12, 64)               0         ['dropout_5[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 12, 64)               128       ['tf.__operators__.add_3[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 12, 64)               66368     ['layer_normalization_4[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 12, 64)               0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, 12, 64)               0         ['dropout_6[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 12, 64)               128       ['tf.__operators__.add_4[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 12, 128)              8320      ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 12, 128)              0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 12, 64)               8256      ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 12, 64)               0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TF  (None, 12, 64)               0         ['dropout_8[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 12, 64)               128       ['tf.__operators__.add_5[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 12, 64)               66368     ['layer_normalization_6[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 12, 64)               0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 12, 64)               0         ['dropout_9[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 12, 64)               128       ['tf.__operators__.add_6[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 12, 128)              8320      ['layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 12, 128)              0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 12, 64)               8256      ['dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 12, 64)               0         ['dense_8[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 12, 64)               0         ['dropout_11[0][0]',          \n",
      " OpLambda)                                                           'tf.__operators__.add_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 12, 64)               128       ['tf.__operators__.add_7[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 768)                  0         ['layer_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 64)                   49216     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 14)                   910       ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 386574 (1.47 MB)\n",
      "Trainable params: 386574 (1.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class PatchExtractor(layers.Layer):\n",
    "  \"\"\"Extract 2D patches from images.\"\"\"\n",
    "  def __init__(self, patch_size=(3,7)):\n",
    "    super().__init__()\n",
    "    self.patch_size = patch_size\n",
    "\n",
    "  def call(self, images):\n",
    "    patch_h, patch_w = self.patch_size\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=(1, patch_h, patch_w, 1),\n",
    "        strides=(1, patch_h, patch_w, 1),\n",
    "        rates=(1,1,1,1),\n",
    "        padding='VALID'\n",
    "    )\n",
    "    patch_dims = tf.shape(patches)[-1]\n",
    "    patches = tf.reshape(\n",
    "        patches,\n",
    "        [batch_size, -1, patch_dims]\n",
    "    )\n",
    "    return patches\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "  \"\"\"Linear embedding + learnable positional encoding.\"\"\"\n",
    "  def __init__(self, num_patches, embed_dim):\n",
    "    super().__init__()\n",
    "    self.num_patches = num_patches\n",
    "    self.projection  = layers.Dense(embed_dim)\n",
    "    self.pos_embed   = tf.Variable(\n",
    "        initial_value=tf.zeros((1,num_patches,embed_dim)),\n",
    "        trainable=True,\n",
    "        name=\"pos_embedding\"\n",
    "    )\n",
    "\n",
    "  def call(self, patch_batch):\n",
    "    projected = self.projection(patch_batch)\n",
    "    return projected + self.pos_embed\n",
    "\n",
    "def transformer_encoder(inputs,\n",
    "                        head_size,\n",
    "                        num_heads,\n",
    "                        ff_dim,\n",
    "                        dropout=0.1):\n",
    "  # LayerNorm + Multi-head attention\n",
    "  x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "  x = layers.MultiHeadAttention(num_heads=num_heads,\n",
    "                                key_dim=head_size,\n",
    "                                dropout=dropout)(x, x)\n",
    "  x = layers.Dropout(dropout)(x)\n",
    "  res = x + inputs\n",
    "  \n",
    "  # LN + feed-forward (mlp)\n",
    "  x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "  x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "  x = layers.Dropout(dropout)(x)\n",
    "  x = layers.Dense(inputs.shape[-1], activation=\"linear\")(x)\n",
    "  x = layers.Dropout(dropout)(x)\n",
    "  \n",
    "  return x + res\n",
    "\n",
    "def create_vit_model(input_shape=(13,21,2),\n",
    "                     patch_size=(3,7),\n",
    "                     embed_dim=64,\n",
    "                     num_heads=4,\n",
    "                     ff_dim=128,\n",
    "                     num_layers=4,\n",
    "                     dropout=0.1,\n",
    "                     final_outputs=14):\n",
    "  inputs = layers.Input(shape=input_shape)\n",
    "  patches = PatchExtractor(patch_size=patch_size)(inputs)\n",
    "  \n",
    "  H, W, C = input_shape\n",
    "  ph, pw  = patch_size\n",
    "  num_patches = (H // ph) * (W // pw)\n",
    "\n",
    "  encoded_patches = PatchEncoder(num_patches, embed_dim)(patches)\n",
    "\n",
    "  x = encoded_patches\n",
    "  for _ in range(num_layers):\n",
    "    x = transformer_encoder(x,\n",
    "                            head_size=embed_dim,\n",
    "                            num_heads=num_heads,\n",
    "                            ff_dim=ff_dim,\n",
    "                            dropout=dropout)\n",
    "  \n",
    "  x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "  x = layers.Flatten()(x)\n",
    "  x = layers.Dense(64, activation='relu')(x)\n",
    "  outputs = layers.Dense(final_outputs, activation='linear')(x)\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  return model\n",
    "\n",
    "model = create_vit_model(\n",
    "    input_shape=(13,21,2),   \n",
    "    patch_size=(3,7),        \n",
    "    embed_dim=64,            \n",
    "    num_heads=4,             \n",
    "    ff_dim=128,              \n",
    "    num_layers=4,           \n",
    "    dropout=0.1,             \n",
    "    final_outputs=14         \n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afc6a1a7-b8d6-47a1-95c5-b4a21fefde42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from utils import track_loss_values\n",
    "\n",
    "def custom_loss(y, p_base, minval=1e-9, maxval=1e9, scale = 512, reg_weight=0.5):\n",
    "    \n",
    "    p = p_base\n",
    "    \n",
    "    mu = p[:, 0:8:2]\n",
    "    \n",
    "    # creating each matrix element in 4x4\n",
    "    Mdia = minval + tf.math.maximum(p[:, 1:8:2], 0.0)\n",
    "    Mcov = p[:,8:]\n",
    "    \n",
    "    # placeholder zero element\n",
    "    zeros = tf.zeros_like(Mdia[:,0])\n",
    "    \n",
    "    # assembles scale_tril matrix\n",
    "    row1 = tf.stack([Mdia[:,0],zeros,zeros,zeros])\n",
    "    row2 = tf.stack([Mcov[:,0],Mdia[:,1],zeros,zeros])\n",
    "    row3 = tf.stack([Mcov[:,1],Mcov[:,2],Mdia[:,2],zeros])\n",
    "    row4 = tf.stack([Mcov[:,3],Mcov[:,4],Mcov[:,5],Mdia[:,3]])\n",
    "\n",
    "    scale_tril = tf.transpose(tf.stack([row1,row2,row3,row4]),perm=[2,0,1])\n",
    "\n",
    "    dist = tfp.distributions.MultivariateNormalTriL(loc = mu, scale_tril = scale_tril) \n",
    "    \n",
    "    likelihood = dist.prob(y)  \n",
    "    likelihood = tf.clip_by_value(likelihood,minval,maxval)\n",
    "\n",
    "    NLL = -1*tf.math.log(likelihood)\n",
    "\n",
    "    cov_matrix = tf.matmul(scale_tril, tf.transpose(scale_tril, [0, 2, 1])) \n",
    "    variances = tf.linalg.diag_part(cov_matrix)\n",
    "    stds = tf.sqrt(variances + minval)\n",
    "\n",
    "    sigma_regularizer_1 = tf.reduce_sum(stds, axis=1)\n",
    "\n",
    "    track_loss_values(NLL, reg_weight * sigma_regularizer_1)\n",
    "\n",
    "    total_loss = NLL + (reg_weight * sigma_regularizer_1)\n",
    "    \n",
    "    return tf.keras.backend.sum(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870b8741-98fd-4e1e-bc29-7b9fcbb9b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=1e-3), loss=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce60b630-5e1e-41cd-b9a0-7e9a06e9a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint = '%08x' % random.randrange(16**8)\n",
    "os.makedirs(\"trained_models\", exist_ok=True)\n",
    "base_dir = f'./trained_models/model-{fingerprint}-checkpoints'\n",
    "os.makedirs(base_dir, exist_ok=True)  \n",
    "checkpoint_filepath = base_dir + '/weights.{epoch:02d}-t{loss:.2f}-v{val_loss:.2f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ccc68b0-0d5a-4b14-bebc-0d8e6923e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0f649086\n"
     ]
    }
   ],
   "source": [
    "print(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd97cbbf-a137-4a5c-9796-11de00abfa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, Callback\n",
    "from utils import get_loss_metrics, reset_loss_trackers\n",
    "import csv\n",
    "\n",
    "early_stopping_patience = 50\n",
    "\n",
    "class CustomModelCheckpoint(ModelCheckpoint):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "        checkpoints = [f for f in os.listdir(base_dir) if f.startswith('weights')]\n",
    "        if len(checkpoints) > 1:\n",
    "            checkpoints.sort()\n",
    "            for checkpoint in checkpoints[:-1]:\n",
    "                os.remove(os.path.join(base_dir, checkpoint))\n",
    "\n",
    "class PrintSelectiveEpochLoss(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        reset_loss_trackers()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Obtener mtricas actuales del NLL y el trmino de regularizacin ya ponderado\n",
    "        metrics = get_loss_metrics()\n",
    "        \n",
    "        nll = metrics['nll']\n",
    "        reg = metrics['reg_term'] \n",
    "        total = nll + reg\n",
    "\n",
    "        print(f\" Epoch {epoch + 1} - NLL: {nll:.4f} | Reg (weighted): {reg:.4f} | Total approx: {total:.4f}\")\n",
    "\n",
    "es = EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "mcp = CustomModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print_epoch_loss = PrintSelectiveEpochLoss()\n",
    "\n",
    "csv_logger = CSVLogger(f'{base_dir}/training_log.csv', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d3ecb0d-5db2-4610-bd58-e55d06f049a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 21:41:39.201102: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 21:41:46.553127: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x561180daaaf0\n",
      "2025-04-16 21:41:46.934234: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f84b30613b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-16 21:41:46.934296: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB MIG 7g.40gb, Compute Capability 8.0\n",
      "2025-04-16 21:41:46.940538: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-04-16 21:41:46.970252: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744832507.069571  674412 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499/499 [==============================] - ETA: 0s - loss: 13376.2285\n",
      "Epoch 1: val_loss improved from inf to -1835.83594, saving model to ./trained_models/model-0f649086-checkpoints/weights.01-t13376.23-v-1835.84.hdf5\n",
      " Epoch 1 - NLL: 1.3292 | Reg (weighted): 0.8462 | Total approx: 2.1754\n",
      "499/499 [==============================] - 67s 109ms/step - loss: 13376.2285 - val_loss: -1835.8359\n",
      "Epoch 2/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -10932.8594\n",
      "Epoch 2: val_loss improved from -1835.83594 to -23841.68555, saving model to ./trained_models/model-0f649086-checkpoints/weights.02-t-10932.86-v-23841.69.hdf5\n",
      " Epoch 2 - NLL: -2.9243 | Reg (weighted): 0.3062 | Total approx: -2.6181\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -10932.8594 - val_loss: -23841.6855\n",
      "Epoch 3/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -22700.0527\n",
      "Epoch 3: val_loss improved from -23841.68555 to -28022.22070, saving model to ./trained_models/model-0f649086-checkpoints/weights.03-t-22700.05-v-28022.22.hdf5\n",
      " Epoch 3 - NLL: -4.8881 | Reg (weighted): 0.1651 | Total approx: -4.7230\n",
      "499/499 [==============================] - 43s 86ms/step - loss: -22700.0527 - val_loss: -28022.2207\n",
      "Epoch 4/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -25761.4180\n",
      "Epoch 4: val_loss improved from -28022.22070 to -29904.74414, saving model to ./trained_models/model-0f649086-checkpoints/weights.04-t-25761.42-v-29904.74.hdf5\n",
      " Epoch 4 - NLL: -5.4426 | Reg (weighted): 0.1455 | Total approx: -5.2970\n",
      "499/499 [==============================] - 55s 111ms/step - loss: -25761.4180 - val_loss: -29904.7441\n",
      "Epoch 5/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -27932.5645\n",
      "Epoch 5: val_loss improved from -29904.74414 to -30720.60352, saving model to ./trained_models/model-0f649086-checkpoints/weights.05-t-27932.56-v-30720.60.hdf5\n",
      " Epoch 5 - NLL: -5.8192 | Reg (weighted): 0.1321 | Total approx: -5.6871\n",
      "499/499 [==============================] - 47s 94ms/step - loss: -27932.5645 - val_loss: -30720.6035\n",
      "Epoch 6/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -29992.5430\n",
      "Epoch 6: val_loss did not improve from -30720.60352\n",
      " Epoch 6 - NLL: -6.0607 | Reg (weighted): 0.1213 | Total approx: -5.9393\n",
      "499/499 [==============================] - 45s 90ms/step - loss: -29992.5430 - val_loss: -27944.2930\n",
      "Epoch 7/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -30436.2852\n",
      "Epoch 7: val_loss improved from -30720.60352 to -34139.61719, saving model to ./trained_models/model-0f649086-checkpoints/weights.07-t-30436.29-v-34139.62.hdf5\n",
      " Epoch 7 - NLL: -6.3380 | Reg (weighted): 0.1191 | Total approx: -6.2189\n",
      "499/499 [==============================] - 56s 112ms/step - loss: -30436.2852 - val_loss: -34139.6172\n",
      "Epoch 8/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -31498.1387\n",
      "Epoch 8: val_loss improved from -34139.61719 to -35359.76172, saving model to ./trained_models/model-0f649086-checkpoints/weights.08-t-31498.14-v-35359.76.hdf5\n",
      " Epoch 8 - NLL: -6.5498 | Reg (weighted): 0.1129 | Total approx: -6.4368\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -31498.1387 - val_loss: -35359.7617\n",
      "Epoch 9/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -32443.5020\n",
      "Epoch 9: val_loss did not improve from -35359.76172\n",
      " Epoch 9 - NLL: -6.5864 | Reg (weighted): 0.1139 | Total approx: -6.4725\n",
      "499/499 [==============================] - 45s 90ms/step - loss: -32443.5020 - val_loss: -31671.1328\n",
      "Epoch 10/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -32509.2539\n",
      "Epoch 10: val_loss improved from -35359.76172 to -35529.98828, saving model to ./trained_models/model-0f649086-checkpoints/weights.10-t-32509.25-v-35529.99.hdf5\n",
      " Epoch 10 - NLL: -6.7205 | Reg (weighted): 0.1091 | Total approx: -6.6114\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -32509.2539 - val_loss: -35529.9883\n",
      "Epoch 11/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -33306.2773\n",
      "Epoch 11: val_loss improved from -35529.98828 to -36734.74609, saving model to ./trained_models/model-0f649086-checkpoints/weights.11-t-33306.28-v-36734.75.hdf5\n",
      " Epoch 11 - NLL: -6.8891 | Reg (weighted): 0.1043 | Total approx: -6.7847\n",
      "499/499 [==============================] - 51s 103ms/step - loss: -33306.2773 - val_loss: -36734.7461\n",
      "Epoch 12/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -34044.2773\n",
      "Epoch 12: val_loss improved from -36734.74609 to -37091.72266, saving model to ./trained_models/model-0f649086-checkpoints/weights.12-t-34044.28-v-37091.72.hdf5\n",
      " Epoch 12 - NLL: -7.0210 | Reg (weighted): 0.1011 | Total approx: -6.9199\n",
      "499/499 [==============================] - 43s 87ms/step - loss: -34044.2773 - val_loss: -37091.7227\n",
      "Epoch 13/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -34367.0117\n",
      "Epoch 13: val_loss improved from -37091.72266 to -37540.87109, saving model to ./trained_models/model-0f649086-checkpoints/weights.13-t-34367.01-v-37540.87.hdf5\n",
      " Epoch 13 - NLL: -7.0885 | Reg (weighted): 0.1000 | Total approx: -6.9886\n",
      "499/499 [==============================] - 56s 113ms/step - loss: -34367.0117 - val_loss: -37540.8711\n",
      "Epoch 14/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -34573.5938\n",
      "Epoch 14: val_loss improved from -37540.87109 to -37673.16797, saving model to ./trained_models/model-0f649086-checkpoints/weights.14-t-34573.59-v-37673.17.hdf5\n",
      " Epoch 14 - NLL: -7.1266 | Reg (weighted): 0.0991 | Total approx: -7.0276\n",
      "499/499 [==============================] - 51s 101ms/step - loss: -34573.5938 - val_loss: -37673.1680\n",
      "Epoch 15/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -35477.1992\n",
      "Epoch 15: val_loss did not improve from -37673.16797\n",
      " Epoch 15 - NLL: -7.2529 | Reg (weighted): 0.0962 | Total approx: -7.1567\n",
      "499/499 [==============================] - 43s 86ms/step - loss: -35477.1992 - val_loss: -37014.1328\n",
      "Epoch 16/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -35419.4102\n",
      "Epoch 16: val_loss improved from -37673.16797 to -38703.27734, saving model to ./trained_models/model-0f649086-checkpoints/weights.16-t-35419.41-v-38703.28.hdf5\n",
      " Epoch 16 - NLL: -7.2989 | Reg (weighted): 0.0958 | Total approx: -7.2031\n",
      "499/499 [==============================] - 44s 89ms/step - loss: -35419.4102 - val_loss: -38703.2773\n",
      "Epoch 17/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -34590.9570\n",
      "Epoch 17: val_loss did not improve from -38703.27734\n",
      " Epoch 17 - NLL: -7.1392 | Reg (weighted): 0.1003 | Total approx: -7.0389\n",
      "499/499 [==============================] - 56s 112ms/step - loss: -34590.9570 - val_loss: -37928.5391\n",
      "Epoch 18/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -36335.2461\n",
      "Epoch 18: val_loss did not improve from -38703.27734\n",
      " Epoch 18 - NLL: -7.4460 | Reg (weighted): 0.0918 | Total approx: -7.3542\n",
      "499/499 [==============================] - 47s 93ms/step - loss: -36335.2461 - val_loss: -38645.7344\n",
      "Epoch 19/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -36098.0742\n",
      "Epoch 19: val_loss improved from -38703.27734 to -39329.15625, saving model to ./trained_models/model-0f649086-checkpoints/weights.19-t-36098.07-v-39329.16.hdf5\n",
      " Epoch 19 - NLL: -7.4302 | Reg (weighted): 0.0929 | Total approx: -7.3373\n",
      "499/499 [==============================] - 44s 89ms/step - loss: -36098.0742 - val_loss: -39329.1562\n",
      "Epoch 20/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -35668.4922\n",
      "Epoch 20: val_loss improved from -39329.15625 to -39351.94531, saving model to ./trained_models/model-0f649086-checkpoints/weights.20-t-35668.49-v-39351.95.hdf5\n",
      " Epoch 20 - NLL: -7.3627 | Reg (weighted): 0.0964 | Total approx: -7.2663\n",
      "499/499 [==============================] - 55s 109ms/step - loss: -35668.4922 - val_loss: -39351.9453\n",
      "Epoch 21/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -36888.0039\n",
      "Epoch 21: val_loss did not improve from -39351.94531\n",
      " Epoch 21 - NLL: -7.5570 | Reg (weighted): 0.0903 | Total approx: -7.4667\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -36888.0039 - val_loss: -39250.7344\n",
      "Epoch 22/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -36143.2617\n",
      "Epoch 22: val_loss improved from -39351.94531 to -40092.81250, saving model to ./trained_models/model-0f649086-checkpoints/weights.22-t-36143.26-v-40092.81.hdf5\n",
      " Epoch 22 - NLL: -7.4635 | Reg (weighted): 0.0933 | Total approx: -7.3702\n",
      "499/499 [==============================] - 43s 86ms/step - loss: -36143.2617 - val_loss: -40092.8125\n",
      "Epoch 23/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -36986.8945\n",
      "Epoch 23: val_loss did not improve from -40092.81250\n",
      " Epoch 23 - NLL: -7.5657 | Reg (weighted): 0.0904 | Total approx: -7.4753\n",
      "499/499 [==============================] - 59s 118ms/step - loss: -36986.8945 - val_loss: -39011.5156\n",
      "Epoch 24/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -37548.7578\n",
      "Epoch 24: val_loss did not improve from -40092.81250\n",
      " Epoch 24 - NLL: -7.6898 | Reg (weighted): 0.0878 | Total approx: -7.6020\n",
      "499/499 [==============================] - 55s 109ms/step - loss: -37548.7578 - val_loss: -39999.7930\n",
      "Epoch 25/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -37338.1367\n",
      "Epoch 25: val_loss improved from -40092.81250 to -40197.22266, saving model to ./trained_models/model-0f649086-checkpoints/weights.25-t-37338.14-v-40197.22.hdf5\n",
      " Epoch 25 - NLL: -7.6619 | Reg (weighted): 0.0886 | Total approx: -7.5733\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -37338.1367 - val_loss: -40197.2227\n",
      "Epoch 26/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -37741.3984\n",
      "Epoch 26: val_loss improved from -40197.22266 to -40441.01953, saving model to ./trained_models/model-0f649086-checkpoints/weights.26-t-37741.40-v-40441.02.hdf5\n",
      " Epoch 26 - NLL: -7.7360 | Reg (weighted): 0.0872 | Total approx: -7.6488\n",
      "499/499 [==============================] - 54s 107ms/step - loss: -37741.3984 - val_loss: -40441.0195\n",
      "Epoch 27/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -37844.1953\n",
      "Epoch 27: val_loss improved from -40441.01953 to -40701.53125, saving model to ./trained_models/model-0f649086-checkpoints/weights.27-t-37844.20-v-40701.53.hdf5\n",
      " Epoch 27 - NLL: -7.7608 | Reg (weighted): 0.0862 | Total approx: -7.6746\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -37844.1953 - val_loss: -40701.5312\n",
      "Epoch 28/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -37657.4883\n",
      "Epoch 28: val_loss did not improve from -40701.53125\n",
      " Epoch 28 - NLL: -7.7086 | Reg (weighted): 0.0870 | Total approx: -7.6216\n",
      "499/499 [==============================] - 44s 87ms/step - loss: -37657.4883 - val_loss: -40048.3828\n",
      "Epoch 29/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -38051.2266\n",
      "Epoch 29: val_loss did not improve from -40701.53125\n",
      " Epoch 29 - NLL: -7.7740 | Reg (weighted): 0.0857 | Total approx: -7.6883\n",
      "499/499 [==============================] - 57s 114ms/step - loss: -38051.2266 - val_loss: -40071.6914\n",
      "Epoch 30/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -38126.7461\n",
      "Epoch 30: val_loss did not improve from -40701.53125\n",
      " Epoch 30 - NLL: -7.7852 | Reg (weighted): 0.0863 | Total approx: -7.6989\n",
      "499/499 [==============================] - 56s 112ms/step - loss: -38126.7461 - val_loss: -40010.4023\n",
      "Epoch 31/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -37292.1406\n",
      "Epoch 31: val_loss did not improve from -40701.53125\n",
      " Epoch 31 - NLL: -7.6564 | Reg (weighted): 0.0894 | Total approx: -7.5670\n",
      "499/499 [==============================] - 49s 97ms/step - loss: -37292.1406 - val_loss: -40240.6289\n",
      "Epoch 32/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -38615.1406\n",
      "Epoch 32: val_loss improved from -40701.53125 to -40984.18750, saving model to ./trained_models/model-0f649086-checkpoints/weights.32-t-38615.14-v-40984.19.hdf5\n",
      " Epoch 32 - NLL: -7.8957 | Reg (weighted): 0.0829 | Total approx: -7.8129\n",
      "499/499 [==============================] - 55s 111ms/step - loss: -38615.1406 - val_loss: -40984.1875\n",
      "Epoch 33/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -38780.6680\n",
      "Epoch 33: val_loss improved from -40984.18750 to -41326.04297, saving model to ./trained_models/model-0f649086-checkpoints/weights.33-t-38780.67-v-41326.04.hdf5\n",
      " Epoch 33 - NLL: -7.9346 | Reg (weighted): 0.0828 | Total approx: -7.8518\n",
      "499/499 [==============================] - 47s 93ms/step - loss: -38780.6680 - val_loss: -41326.0430\n",
      "Epoch 34/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -38236.2031\n",
      "Epoch 34: val_loss did not improve from -41326.04297\n",
      " Epoch 34 - NLL: -7.5670 | Reg (weighted): 0.0943 | Total approx: -7.4727\n",
      "499/499 [==============================] - 45s 89ms/step - loss: -38236.2031 - val_loss: -32639.1465\n",
      "Epoch 35/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -38742.1562\n",
      "Epoch 35: val_loss did not improve from -41326.04297\n",
      " Epoch 35 - NLL: -7.9186 | Reg (weighted): 0.0830 | Total approx: -7.8356\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -38742.1562 - val_loss: -41031.4961\n",
      "Epoch 36/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -38309.6992\n",
      "Epoch 36: val_loss did not improve from -41326.04297\n",
      " Epoch 36 - NLL: -7.8492 | Reg (weighted): 0.0845 | Total approx: -7.7646\n",
      "499/499 [==============================] - 56s 112ms/step - loss: -38309.6992 - val_loss: -41068.7070\n",
      "Epoch 37/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -39179.1680\n",
      "Epoch 37: val_loss improved from -41326.04297 to -41537.80859, saving model to ./trained_models/model-0f649086-checkpoints/weights.37-t-39179.17-v-41537.81.hdf5\n",
      " Epoch 37 - NLL: -8.0065 | Reg (weighted): 0.0811 | Total approx: -7.9254\n",
      "499/499 [==============================] - 43s 86ms/step - loss: -39179.1680 - val_loss: -41537.8086\n",
      "Epoch 38/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -39329.5469\n",
      "Epoch 38: val_loss did not improve from -41537.80859\n",
      " Epoch 38 - NLL: -8.0260 | Reg (weighted): 0.0814 | Total approx: -7.9447\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -39329.5469 - val_loss: -41359.9180\n",
      "Epoch 39/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -38821.9297\n",
      "Epoch 39: val_loss did not improve from -41537.80859\n",
      " Epoch 39 - NLL: -7.9420 | Reg (weighted): 0.0830 | Total approx: -7.8590\n",
      "499/499 [==============================] - 50s 101ms/step - loss: -38821.9297 - val_loss: -41333.3203\n",
      "Epoch 40/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -39123.6562\n",
      "Epoch 40: val_loss did not improve from -41537.80859\n",
      " Epoch 40 - NLL: -7.9956 | Reg (weighted): 0.0819 | Total approx: -7.9138\n",
      "499/499 [==============================] - 41s 83ms/step - loss: -39123.6562 - val_loss: -41468.8008\n",
      "Epoch 41/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -38726.3867\n",
      "Epoch 41: val_loss improved from -41537.80859 to -41653.83594, saving model to ./trained_models/model-0f649086-checkpoints/weights.41-t-38726.39-v-41653.84.hdf5\n",
      " Epoch 41 - NLL: -7.9380 | Reg (weighted): 0.0842 | Total approx: -7.8537\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -38726.3867 - val_loss: -41653.8359\n",
      "Epoch 42/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -38921.6016\n",
      "Epoch 42: val_loss improved from -41653.83594 to -41874.72656, saving model to ./trained_models/model-0f649086-checkpoints/weights.42-t-38921.60-v-41874.73.hdf5\n",
      " Epoch 42 - NLL: -7.9756 | Reg (weighted): 0.0820 | Total approx: -7.8936\n",
      "499/499 [==============================] - 50s 100ms/step - loss: -38921.6016 - val_loss: -41874.7266\n",
      "Epoch 43/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -39633.5742\n",
      "Epoch 43: val_loss improved from -41874.72656 to -42141.76953, saving model to ./trained_models/model-0f649086-checkpoints/weights.43-t-39633.57-v-42141.77.hdf5\n",
      " Epoch 43 - NLL: -8.1016 | Reg (weighted): 0.0802 | Total approx: -8.0214\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -39633.5742 - val_loss: -42141.7695\n",
      "Epoch 44/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -39297.8320\n",
      "Epoch 44: val_loss did not improve from -42141.76953\n",
      " Epoch 44 - NLL: -8.0343 | Reg (weighted): 0.0813 | Total approx: -7.9529\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -39297.8320 - val_loss: -41769.5391\n",
      "Epoch 45/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -39557.8242\n",
      "Epoch 45: val_loss improved from -42141.76953 to -42160.99609, saving model to ./trained_models/model-0f649086-checkpoints/weights.45-t-39557.82-v-42161.00.hdf5\n",
      " Epoch 45 - NLL: -8.0896 | Reg (weighted): 0.0801 | Total approx: -8.0094\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -39557.8242 - val_loss: -42160.9961\n",
      "Epoch 46/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -39393.5781\n",
      "Epoch 46: val_loss did not improve from -42160.99609\n",
      " Epoch 46 - NLL: -8.0616 | Reg (weighted): 0.0812 | Total approx: -7.9804\n",
      "499/499 [==============================] - 42s 84ms/step - loss: -39393.5781 - val_loss: -42114.3945\n",
      "Epoch 47/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -39445.1094\n",
      "Epoch 47: val_loss improved from -42160.99609 to -42231.92969, saving model to ./trained_models/model-0f649086-checkpoints/weights.47-t-39445.11-v-42231.93.hdf5\n",
      " Epoch 47 - NLL: -8.0737 | Reg (weighted): 0.0808 | Total approx: -7.9929\n",
      "499/499 [==============================] - 53s 105ms/step - loss: -39445.1094 - val_loss: -42231.9297\n",
      "Epoch 48/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -39762.3789\n",
      "Epoch 48: val_loss did not improve from -42231.92969\n",
      " Epoch 48 - NLL: -8.1237 | Reg (weighted): 0.0794 | Total approx: -8.0442\n",
      "499/499 [==============================] - 44s 89ms/step - loss: -39762.3789 - val_loss: -42181.0859\n",
      "Epoch 49/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -39708.5625\n",
      "Epoch 49: val_loss improved from -42231.92969 to -42260.12500, saving model to ./trained_models/model-0f649086-checkpoints/weights.49-t-39708.56-v-42260.12.hdf5\n",
      " Epoch 49 - NLL: -8.1178 | Reg (weighted): 0.0799 | Total approx: -8.0379\n",
      "499/499 [==============================] - 55s 110ms/step - loss: -39708.5625 - val_loss: -42260.1250\n",
      "Epoch 50/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -39618.5312\n",
      "Epoch 50: val_loss improved from -42260.12500 to -42409.49609, saving model to ./trained_models/model-0f649086-checkpoints/weights.50-t-39618.53-v-42409.50.hdf5\n",
      " Epoch 50 - NLL: -8.1080 | Reg (weighted): 0.0803 | Total approx: -8.0277\n",
      "499/499 [==============================] - 45s 91ms/step - loss: -39618.5312 - val_loss: -42409.4961\n",
      "Epoch 51/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40088.0117\n",
      "Epoch 51: val_loss did not improve from -42409.49609\n",
      " Epoch 51 - NLL: -8.1779 | Reg (weighted): 0.0779 | Total approx: -8.1000\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -40088.0117 - val_loss: -42221.2734\n",
      "Epoch 52/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -39991.7227\n",
      "Epoch 52: val_loss did not improve from -42409.49609\n",
      " Epoch 52 - NLL: -8.1685 | Reg (weighted): 0.0792 | Total approx: -8.0893\n",
      "499/499 [==============================] - 44s 89ms/step - loss: -39991.7227 - val_loss: -42381.8242\n",
      "Epoch 53/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40129.5352\n",
      "Epoch 53: val_loss did not improve from -42409.49609\n",
      " Epoch 53 - NLL: -8.1750 | Reg (weighted): 0.0784 | Total approx: -8.0965\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -40129.5352 - val_loss: -41908.6758\n",
      "Epoch 54/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -39712.7656\n",
      "Epoch 54: val_loss improved from -42409.49609 to -42845.74609, saving model to ./trained_models/model-0f649086-checkpoints/weights.54-t-39712.77-v-42845.75.hdf5\n",
      " Epoch 54 - NLL: -8.1377 | Reg (weighted): 0.0796 | Total approx: -8.0581\n",
      "499/499 [==============================] - 45s 90ms/step - loss: -39712.7656 - val_loss: -42845.7461\n",
      "Epoch 55/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40327.1445\n",
      "Epoch 55: val_loss did not improve from -42845.74609\n",
      " Epoch 55 - NLL: -8.2296 | Reg (weighted): 0.0782 | Total approx: -8.1514\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -40327.1445 - val_loss: -42564.2734\n",
      "Epoch 56/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40000.5234\n",
      "Epoch 56: val_loss did not improve from -42845.74609\n",
      " Epoch 56 - NLL: -8.1759 | Reg (weighted): 0.0789 | Total approx: -8.0970\n",
      "499/499 [==============================] - 56s 111ms/step - loss: -40000.5234 - val_loss: -42567.9062\n",
      "Epoch 57/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40251.3359\n",
      "Epoch 57: val_loss did not improve from -42845.74609\n",
      " Epoch 57 - NLL: -8.2144 | Reg (weighted): 0.0780 | Total approx: -8.1364\n",
      "499/499 [==============================] - 52s 103ms/step - loss: -40251.3359 - val_loss: -42493.9258\n",
      "Epoch 58/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40332.2695\n",
      "Epoch 58: val_loss did not improve from -42845.74609\n",
      " Epoch 58 - NLL: -8.2177 | Reg (weighted): 0.0781 | Total approx: -8.1396\n",
      "499/499 [==============================] - 50s 101ms/step - loss: -40332.2695 - val_loss: -42182.4492\n",
      "Epoch 59/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40377.4414\n",
      "Epoch 59: val_loss improved from -42845.74609 to -43121.42969, saving model to ./trained_models/model-0f649086-checkpoints/weights.59-t-40377.44-v-43121.43.hdf5\n",
      " Epoch 59 - NLL: -8.2554 | Reg (weighted): 0.0772 | Total approx: -8.1783\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -40377.4414 - val_loss: -43121.4297\n",
      "Epoch 60/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40620.5117\n",
      "Epoch 60: val_loss did not improve from -43121.42969\n",
      " Epoch 60 - NLL: -8.2798 | Reg (weighted): 0.0771 | Total approx: -8.2027\n",
      "499/499 [==============================] - 44s 88ms/step - loss: -40620.5117 - val_loss: -42634.8477\n",
      "Epoch 61/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -39051.0273\n",
      "Epoch 61: val_loss did not improve from -43121.42969\n",
      " Epoch 61 - NLL: -8.0260 | Reg (weighted): 0.0853 | Total approx: -7.9407\n",
      "499/499 [==============================] - 47s 94ms/step - loss: -39051.0273 - val_loss: -42646.3594\n",
      "Epoch 62/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40583.6797\n",
      "Epoch 62: val_loss did not improve from -43121.42969\n",
      " Epoch 62 - NLL: -8.2876 | Reg (weighted): 0.0769 | Total approx: -8.2107\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -40583.6797 - val_loss: -43060.3945\n",
      "Epoch 63/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40646.9570\n",
      "Epoch 63: val_loss did not improve from -43121.42969\n",
      " Epoch 63 - NLL: -8.2782 | Reg (weighted): 0.0773 | Total approx: -8.2009\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -40646.9570 - val_loss: -42445.5117\n",
      "Epoch 64/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40089.4961\n",
      "Epoch 64: val_loss improved from -43121.42969 to -43125.83984, saving model to ./trained_models/model-0f649086-checkpoints/weights.64-t-40089.50-v-43125.84.hdf5\n",
      " Epoch 64 - NLL: -8.2089 | Reg (weighted): 0.0786 | Total approx: -8.1303\n",
      "499/499 [==============================] - 50s 100ms/step - loss: -40089.4961 - val_loss: -43125.8398\n",
      "Epoch 65/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40524.5820\n",
      "Epoch 65: val_loss improved from -43125.83984 to -43224.79297, saving model to ./trained_models/model-0f649086-checkpoints/weights.65-t-40524.58-v-43224.79.hdf5\n",
      " Epoch 65 - NLL: -8.2832 | Reg (weighted): 0.0769 | Total approx: -8.2063\n",
      "499/499 [==============================] - 49s 99ms/step - loss: -40524.5820 - val_loss: -43224.7930\n",
      "Epoch 66/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40839.5898\n",
      "Epoch 66: val_loss did not improve from -43224.79297\n",
      " Epoch 66 - NLL: -8.3323 | Reg (weighted): 0.0759 | Total approx: -8.2564\n",
      "499/499 [==============================] - 49s 99ms/step - loss: -40839.5898 - val_loss: -43148.2695\n",
      "Epoch 67/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40716.5938\n",
      "Epoch 67: val_loss did not improve from -43224.79297\n",
      " Epoch 67 - NLL: -8.3068 | Reg (weighted): 0.0765 | Total approx: -8.2303\n",
      "499/499 [==============================] - 44s 88ms/step - loss: -40716.5938 - val_loss: -42981.7500\n",
      "Epoch 68/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40763.2305\n",
      "Epoch 68: val_loss did not improve from -43224.79297\n",
      " Epoch 68 - NLL: -8.1976 | Reg (weighted): 0.0786 | Total approx: -8.1189\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -40763.2305 - val_loss: -39389.8359\n",
      "Epoch 69/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40788.5000\n",
      "Epoch 69: val_loss did not improve from -43224.79297\n",
      " Epoch 69 - NLL: -8.3201 | Reg (weighted): 0.0762 | Total approx: -8.2439\n",
      "499/499 [==============================] - 50s 100ms/step - loss: -40788.5000 - val_loss: -43030.0703\n",
      "Epoch 70/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40811.4492\n",
      "Epoch 70: val_loss did not improve from -43224.79297\n",
      " Epoch 70 - NLL: -8.3060 | Reg (weighted): 0.0762 | Total approx: -8.2298\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -40811.4492 - val_loss: -42489.5625\n",
      "Epoch 71/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40837.6875\n",
      "Epoch 71: val_loss did not improve from -43224.79297\n",
      " Epoch 71 - NLL: -8.3298 | Reg (weighted): 0.0759 | Total approx: -8.2539\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -40837.6875 - val_loss: -43082.6484\n",
      "Epoch 72/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40999.9414\n",
      "Epoch 72: val_loss did not improve from -43224.79297\n",
      " Epoch 72 - NLL: -8.3544 | Reg (weighted): 0.0753 | Total approx: -8.2791\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -40999.9414 - val_loss: -43024.1602\n",
      "Epoch 73/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40936.7891\n",
      "Epoch 73: val_loss did not improve from -43224.79297\n",
      " Epoch 73 - NLL: -8.3063 | Reg (weighted): 0.0765 | Total approx: -8.2298\n",
      "499/499 [==============================] - 45s 89ms/step - loss: -40936.7891 - val_loss: -41857.4961\n",
      "Epoch 74/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -41047.7578\n",
      "Epoch 74: val_loss did not improve from -43224.79297\n",
      " Epoch 74 - NLL: -8.3631 | Reg (weighted): 0.0754 | Total approx: -8.2877\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -41047.7578 - val_loss: -43045.2500\n",
      "Epoch 75/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -41180.5000\n",
      "Epoch 75: val_loss improved from -43224.79297 to -43429.24219, saving model to ./trained_models/model-0f649086-checkpoints/weights.75-t-41180.50-v-43429.24.hdf5\n",
      " Epoch 75 - NLL: -8.3977 | Reg (weighted): 0.0750 | Total approx: -8.3226\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -41180.5000 - val_loss: -43429.2422\n",
      "Epoch 76/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40798.5625\n",
      "Epoch 76: val_loss did not improve from -43429.24219\n",
      " Epoch 76 - NLL: -8.3275 | Reg (weighted): 0.0762 | Total approx: -8.2512\n",
      "499/499 [==============================] - 57s 114ms/step - loss: -40798.5625 - val_loss: -43199.9766\n",
      "Epoch 77/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40712.0664\n",
      "Epoch 77: val_loss did not improve from -43429.24219\n",
      " Epoch 77 - NLL: -8.3098 | Reg (weighted): 0.0761 | Total approx: -8.2337\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -40712.0664 - val_loss: -43109.9023\n",
      "Epoch 78/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -41166.7266\n",
      "Epoch 78: val_loss did not improve from -43429.24219\n",
      " Epoch 78 - NLL: -8.3849 | Reg (weighted): 0.0752 | Total approx: -8.3097\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -41166.7266 - val_loss: -43107.1719\n",
      "Epoch 79/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -41286.1875\n",
      "Epoch 79: val_loss did not improve from -43429.24219\n",
      " Epoch 79 - NLL: -8.4130 | Reg (weighted): 0.0741 | Total approx: -8.3388\n",
      "499/499 [==============================] - 44s 88ms/step - loss: -41286.1875 - val_loss: -43384.3398\n",
      "Epoch 80/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40393.1719\n",
      "Epoch 80: val_loss improved from -43429.24219 to -43504.91406, saving model to ./trained_models/model-0f649086-checkpoints/weights.80-t-40393.17-v-43504.91.hdf5\n",
      " Epoch 80 - NLL: -8.2722 | Reg (weighted): 0.0786 | Total approx: -8.1935\n",
      "499/499 [==============================] - 49s 99ms/step - loss: -40393.1719 - val_loss: -43504.9141\n",
      "Epoch 81/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -41341.1836\n",
      "Epoch 81: val_loss improved from -43504.91406 to -43832.22656, saving model to ./trained_models/model-0f649086-checkpoints/weights.81-t-41341.18-v-43832.23.hdf5\n",
      " Epoch 81 - NLL: -8.4371 | Reg (weighted): 0.0742 | Total approx: -8.3629\n",
      "499/499 [==============================] - 56s 111ms/step - loss: -41341.1836 - val_loss: -43832.2266\n",
      "Epoch 82/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -40741.0820\n",
      "Epoch 82: val_loss did not improve from -43832.22656\n",
      " Epoch 82 - NLL: -8.3196 | Reg (weighted): 0.0770 | Total approx: -8.2426\n",
      "499/499 [==============================] - 59s 117ms/step - loss: -40741.0820 - val_loss: -43231.5508\n",
      "Epoch 83/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -41409.2109\n",
      "Epoch 83: val_loss did not improve from -43832.22656\n",
      " Epoch 83 - NLL: -8.4363 | Reg (weighted): 0.0745 | Total approx: -8.3618\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -41409.2109 - val_loss: -43456.6602\n",
      "Epoch 84/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -41321.4531\n",
      "Epoch 84: val_loss did not improve from -43832.22656\n",
      " Epoch 84 - NLL: -8.4172 | Reg (weighted): 0.0744 | Total approx: -8.3428\n",
      "499/499 [==============================] - 44s 88ms/step - loss: -41321.4531 - val_loss: -43326.9883\n",
      "Epoch 85/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -41460.9492\n",
      "Epoch 85: val_loss did not improve from -43832.22656\n",
      " Epoch 85 - NLL: -8.4439 | Reg (weighted): 0.0738 | Total approx: -8.3701\n",
      "499/499 [==============================] - 53s 105ms/step - loss: -41460.9492 - val_loss: -43446.3906\n",
      "Epoch 86/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -41135.9180\n",
      "Epoch 86: val_loss did not improve from -43832.22656\n",
      " Epoch 86 - NLL: -8.3726 | Reg (weighted): 0.0750 | Total approx: -8.2975\n",
      "499/499 [==============================] - 65s 130ms/step - loss: -41135.9180 - val_loss: -42896.1016\n",
      "Epoch 87/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -41500.2969\n",
      "Epoch 87: val_loss did not improve from -43832.22656\n",
      " Epoch 87 - NLL: -8.4091 | Reg (weighted): 0.0740 | Total approx: -8.3351\n",
      "499/499 [==============================] - 51s 103ms/step - loss: -41500.2969 - val_loss: -42191.4922\n",
      "Epoch 88/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -41471.8359\n",
      "Epoch 88: val_loss did not improve from -43832.22656\n",
      " Epoch 88 - NLL: -8.4552 | Reg (weighted): 0.0740 | Total approx: -8.3812\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -41471.8359 - val_loss: -43725.9023\n",
      "Epoch 89/1000\n",
      "171/499 [=========>....................] - ETA: 29s - loss: -41432.3867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 136: val_loss did not improve from -44566.09766\n",
      " Epoch 136 - NLL: -8.6213 | Reg (weighted): 0.0710 | Total approx: -8.5504\n",
      "499/499 [==============================] - 46s 91ms/step - loss: -42414.6797 - val_loss: -44076.0469\n",
      "Epoch 137/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42508.9844\n",
      "Epoch 137: val_loss did not improve from -44566.09766\n",
      " Epoch 137 - NLL: -8.6397 | Reg (weighted): 0.0711 | Total approx: -8.5686\n",
      "499/499 [==============================] - 46s 91ms/step - loss: -42508.9844 - val_loss: -44149.1250\n",
      "Epoch 138/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42213.5586\n",
      "Epoch 138: val_loss did not improve from -44566.09766\n",
      " Epoch 138 - NLL: -8.5878 | Reg (weighted): 0.0720 | Total approx: -8.5159\n",
      "499/499 [==============================] - 62s 125ms/step - loss: -42213.5586 - val_loss: -44048.1094\n",
      "Epoch 139/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42522.4883\n",
      "Epoch 139: val_loss improved from -44566.09766 to -44662.66406, saving model to ./trained_models/model-0f649086-checkpoints/weights.139-t-42522.49-v-44662.66.hdf5\n",
      " Epoch 139 - NLL: -8.6587 | Reg (weighted): 0.0708 | Total approx: -8.5878\n",
      "499/499 [==============================] - 65s 131ms/step - loss: -42522.4883 - val_loss: -44662.6641\n",
      "Epoch 140/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42500.1289\n",
      "Epoch 140: val_loss did not improve from -44662.66406\n",
      " Epoch 140 - NLL: -8.6475 | Reg (weighted): 0.0711 | Total approx: -8.5764\n",
      "499/499 [==============================] - 57s 113ms/step - loss: -42500.1289 - val_loss: -44431.2109\n",
      "Epoch 141/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42531.9805\n",
      "Epoch 141: val_loss did not improve from -44662.66406\n",
      " Epoch 141 - NLL: -8.6519 | Reg (weighted): 0.0709 | Total approx: -8.5810\n",
      "499/499 [==============================] - 67s 133ms/step - loss: -42531.9805 - val_loss: -44408.8555\n",
      "Epoch 142/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42464.2305\n",
      "Epoch 142: val_loss did not improve from -44662.66406\n",
      " Epoch 142 - NLL: -8.6430 | Reg (weighted): 0.0713 | Total approx: -8.5717\n",
      "499/499 [==============================] - 61s 123ms/step - loss: -42464.2305 - val_loss: -44470.2383\n",
      "Epoch 143/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42558.6992\n",
      "Epoch 143: val_loss did not improve from -44662.66406\n",
      " Epoch 143 - NLL: -8.6519 | Reg (weighted): 0.0709 | Total approx: -8.5810\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -42558.6992 - val_loss: -44273.4141\n",
      "Epoch 144/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42361.3984\n",
      "Epoch 144: val_loss improved from -44662.66406 to -44728.61328, saving model to ./trained_models/model-0f649086-checkpoints/weights.144-t-42361.40-v-44728.61.hdf5\n",
      " Epoch 144 - NLL: -8.6342 | Reg (weighted): 0.0711 | Total approx: -8.5631\n",
      "499/499 [==============================] - 47s 93ms/step - loss: -42361.3984 - val_loss: -44728.6133\n",
      "Epoch 145/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42591.0156\n",
      "Epoch 145: val_loss did not improve from -44728.61328\n",
      " Epoch 145 - NLL: -8.6629 | Reg (weighted): 0.0708 | Total approx: -8.5921\n",
      "499/499 [==============================] - 55s 110ms/step - loss: -42591.0156 - val_loss: -44445.4883\n",
      "Epoch 146/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42576.3750\n",
      "Epoch 146: val_loss did not improve from -44728.61328\n",
      " Epoch 146 - NLL: -8.6635 | Reg (weighted): 0.0707 | Total approx: -8.5928\n",
      "499/499 [==============================] - 60s 120ms/step - loss: -42576.3750 - val_loss: -44541.2188\n",
      "Epoch 147/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42632.0156\n",
      "Epoch 147: val_loss did not improve from -44728.61328\n",
      " Epoch 147 - NLL: -8.6723 | Reg (weighted): 0.0706 | Total approx: -8.6017\n",
      "499/499 [==============================] - 56s 111ms/step - loss: -42632.0156 - val_loss: -44529.0586\n",
      "Epoch 148/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42610.5391\n",
      "Epoch 148: val_loss did not improve from -44728.61328\n",
      " Epoch 148 - NLL: -8.6635 | Reg (weighted): 0.0704 | Total approx: -8.5930\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -42610.5391 - val_loss: -44375.2930\n",
      "Epoch 149/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42582.2305\n",
      "Epoch 149: val_loss did not improve from -44728.61328\n",
      " Epoch 149 - NLL: -8.6599 | Reg (weighted): 0.0708 | Total approx: -8.5890\n",
      "499/499 [==============================] - 47s 95ms/step - loss: -42582.2305 - val_loss: -44398.0312\n",
      "Epoch 150/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42719.8750\n",
      "Epoch 150: val_loss did not improve from -44728.61328\n",
      " Epoch 150 - NLL: -8.6913 | Reg (weighted): 0.0706 | Total approx: -8.6207\n",
      "499/499 [==============================] - 55s 109ms/step - loss: -42719.8750 - val_loss: -44658.2930\n",
      "Epoch 151/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42705.1797\n",
      "Epoch 151: val_loss did not improve from -44728.61328\n",
      " Epoch 151 - NLL: -8.6787 | Reg (weighted): 0.0707 | Total approx: -8.6081\n",
      "499/499 [==============================] - 57s 114ms/step - loss: -42705.1797 - val_loss: -44351.2461\n",
      "Epoch 152/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42736.2852\n",
      "Epoch 152: val_loss did not improve from -44728.61328\n",
      " Epoch 152 - NLL: -8.6916 | Reg (weighted): 0.0705 | Total approx: -8.6211\n",
      "499/499 [==============================] - 57s 114ms/step - loss: -42736.2852 - val_loss: -44586.4805\n",
      "Epoch 153/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42742.0977\n",
      "Epoch 153: val_loss did not improve from -44728.61328\n",
      " Epoch 153 - NLL: -8.6875 | Reg (weighted): 0.0707 | Total approx: -8.6167\n",
      "499/499 [==============================] - 54s 109ms/step - loss: -42742.0977 - val_loss: -44427.0781\n",
      "Epoch 154/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42696.2852\n",
      "Epoch 154: val_loss did not improve from -44728.61328\n",
      " Epoch 154 - NLL: -8.6736 | Reg (weighted): 0.0702 | Total approx: -8.6033\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -42696.2852 - val_loss: -44254.1484\n",
      "Epoch 155/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42803.4766\n",
      "Epoch 155: val_loss did not improve from -44728.61328\n",
      " Epoch 155 - NLL: -8.7042 | Reg (weighted): 0.0702 | Total approx: -8.6340\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -42803.4766 - val_loss: -44637.2188\n",
      "Epoch 156/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42777.0352\n",
      "Epoch 156: val_loss improved from -44728.61328 to -44813.81250, saving model to ./trained_models/model-0f649086-checkpoints/weights.156-t-42777.04-v-44813.81.hdf5\n",
      " Epoch 156 - NLL: -8.7060 | Reg (weighted): 0.0706 | Total approx: -8.6354\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -42777.0352 - val_loss: -44813.8125\n",
      "Epoch 157/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42734.9883\n",
      "Epoch 157: val_loss did not improve from -44813.81250\n",
      " Epoch 157 - NLL: -8.6952 | Reg (weighted): 0.0710 | Total approx: -8.6242\n",
      "499/499 [==============================] - 54s 109ms/step - loss: -42734.9883 - val_loss: -44688.2148\n",
      "Epoch 158/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42858.6602\n",
      "Epoch 158: val_loss did not improve from -44813.81250\n",
      " Epoch 158 - NLL: -8.7157 | Reg (weighted): 0.0700 | Total approx: -8.6456\n",
      "499/499 [==============================] - 50s 101ms/step - loss: -42858.6602 - val_loss: -44710.7852\n",
      "Epoch 159/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42859.1172\n",
      "Epoch 159: val_loss did not improve from -44813.81250\n",
      " Epoch 159 - NLL: -8.7119 | Reg (weighted): 0.0704 | Total approx: -8.6414\n",
      "499/499 [==============================] - 61s 122ms/step - loss: -42859.1172 - val_loss: -44582.6406\n",
      "Epoch 160/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42906.9844\n",
      "Epoch 160: val_loss did not improve from -44813.81250\n",
      " Epoch 160 - NLL: -8.7236 | Reg (weighted): 0.0705 | Total approx: -8.6531\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -42906.9844 - val_loss: -44691.8672\n",
      "Epoch 161/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42658.5156\n",
      "Epoch 161: val_loss did not improve from -44813.81250\n",
      " Epoch 161 - NLL: -8.6742 | Reg (weighted): 0.0711 | Total approx: -8.6031\n",
      "499/499 [==============================] - 47s 94ms/step - loss: -42658.5156 - val_loss: -44438.1094\n",
      "Epoch 162/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42906.0859\n",
      "Epoch 162: val_loss did not improve from -44813.81250\n",
      " Epoch 162 - NLL: -8.7071 | Reg (weighted): 0.0700 | Total approx: -8.6371\n",
      "499/499 [==============================] - 66s 133ms/step - loss: -42906.0859 - val_loss: -44215.4766\n",
      "Epoch 163/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42937.2188\n",
      "Epoch 163: val_loss improved from -44813.81250 to -44940.45312, saving model to ./trained_models/model-0f649086-checkpoints/weights.163-t-42937.22-v-44940.45.hdf5\n",
      " Epoch 163 - NLL: -8.7363 | Reg (weighted): 0.0699 | Total approx: -8.6664\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -42937.2188 - val_loss: -44940.4531\n",
      "Epoch 164/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42906.7852\n",
      "Epoch 164: val_loss did not improve from -44940.45312\n",
      " Epoch 164 - NLL: -8.7277 | Reg (weighted): 0.0698 | Total approx: -8.6579\n",
      "499/499 [==============================] - 50s 101ms/step - loss: -42906.7852 - val_loss: -44838.6602\n",
      "Epoch 165/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42966.4688\n",
      "Epoch 165: val_loss did not improve from -44940.45312\n",
      " Epoch 165 - NLL: -8.7393 | Reg (weighted): 0.0699 | Total approx: -8.6695\n",
      "499/499 [==============================] - 56s 112ms/step - loss: -42966.4688 - val_loss: -44886.7656\n",
      "Epoch 166/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42974.3984\n",
      "Epoch 166: val_loss did not improve from -44940.45312\n",
      " Epoch 166 - NLL: -8.7275 | Reg (weighted): 0.0697 | Total approx: -8.6578\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -42974.3984 - val_loss: -44494.1484\n",
      "Epoch 167/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42939.7422\n",
      "Epoch 167: val_loss improved from -44940.45312 to -45190.37891, saving model to ./trained_models/model-0f649086-checkpoints/weights.167-t-42939.74-v-45190.38.hdf5\n",
      " Epoch 167 - NLL: -8.7448 | Reg (weighted): 0.0698 | Total approx: -8.6751\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -42939.7422 - val_loss: -45190.3789\n",
      "Epoch 168/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42891.0000\n",
      "Epoch 168: val_loss did not improve from -45190.37891\n",
      " Epoch 168 - NLL: -8.7226 | Reg (weighted): 0.0701 | Total approx: -8.6525\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -42891.0000 - val_loss: -44755.0781\n",
      "Epoch 169/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42996.2344\n",
      "Epoch 169: val_loss did not improve from -45190.37891\n",
      " Epoch 169 - NLL: -8.7459 | Reg (weighted): 0.0700 | Total approx: -8.6759\n",
      "499/499 [==============================] - 56s 111ms/step - loss: -42996.2344 - val_loss: -44931.7500\n",
      "Epoch 170/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43035.6719\n",
      "Epoch 170: val_loss did not improve from -45190.37891\n",
      " Epoch 170 - NLL: -8.7399 | Reg (weighted): 0.0694 | Total approx: -8.6706\n",
      "499/499 [==============================] - 48s 97ms/step - loss: -43035.6719 - val_loss: -44570.6133\n",
      "Epoch 171/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43051.5469\n",
      "Epoch 171: val_loss improved from -45190.37891 to -45334.33984, saving model to ./trained_models/model-0f649086-checkpoints/weights.171-t-43051.55-v-45334.34.hdf5\n",
      " Epoch 171 - NLL: -8.7683 | Reg (weighted): 0.0698 | Total approx: -8.6985\n",
      "499/499 [==============================] - 60s 119ms/step - loss: -43051.5469 - val_loss: -45334.3398\n",
      "Epoch 172/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43054.0859\n",
      "Epoch 172: val_loss did not improve from -45334.33984\n",
      " Epoch 172 - NLL: -8.7503 | Reg (weighted): 0.0699 | Total approx: -8.6803\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -43054.0859 - val_loss: -44771.6680\n",
      "Epoch 173/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43063.6133\n",
      "Epoch 173: val_loss did not improve from -45334.33984\n",
      " Epoch 173 - NLL: -8.7611 | Reg (weighted): 0.0698 | Total approx: -8.6912\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -43063.6133 - val_loss: -45053.6758\n",
      "Epoch 174/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43073.0234\n",
      "Epoch 174: val_loss did not improve from -45334.33984\n",
      " Epoch 174 - NLL: -8.7594 | Reg (weighted): 0.0696 | Total approx: -8.6898\n",
      "499/499 [==============================] - 50s 101ms/step - loss: -43073.0234 - val_loss: -44961.7070\n",
      "Epoch 175/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43066.0742\n",
      "Epoch 175: val_loss did not improve from -45334.33984\n",
      " Epoch 175 - NLL: -8.7491 | Reg (weighted): 0.0699 | Total approx: -8.6793\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -43066.0742 - val_loss: -44680.5859\n",
      "Epoch 176/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42941.5312\n",
      "Epoch 176: val_loss did not improve from -45334.33984\n",
      " Epoch 176 - NLL: -8.7337 | Reg (weighted): 0.0701 | Total approx: -8.6636\n",
      "499/499 [==============================] - 47s 95ms/step - loss: -42941.5312 - val_loss: -44834.7266\n",
      "Epoch 177/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43189.6328\n",
      "Epoch 177: val_loss did not improve from -45334.33984\n",
      " Epoch 177 - NLL: -8.7658 | Reg (weighted): 0.0695 | Total approx: -8.6963\n",
      "499/499 [==============================] - 58s 116ms/step - loss: -43189.6328 - val_loss: -44570.2617\n",
      "Epoch 178/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43100.6797\n",
      "Epoch 178: val_loss did not improve from -45334.33984\n",
      " Epoch 178 - NLL: -8.7666 | Reg (weighted): 0.0696 | Total approx: -8.6970\n",
      "499/499 [==============================] - 51s 103ms/step - loss: -43100.6797 - val_loss: -45040.7969\n",
      "Epoch 179/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43107.6797\n",
      "Epoch 179: val_loss did not improve from -45334.33984\n",
      " Epoch 179 - NLL: -8.7584 | Reg (weighted): 0.0697 | Total approx: -8.6888\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -43107.6797 - val_loss: -44758.0391\n",
      "Epoch 180/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42875.9648\n",
      "Epoch 180: val_loss did not improve from -45334.33984\n",
      " Epoch 180 - NLL: -8.5637 | Reg (weighted): 0.0734 | Total approx: -8.4903\n",
      "499/499 [==============================] - 48s 97ms/step - loss: -42875.9648 - val_loss: -39943.2891\n",
      "Epoch 181/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43034.7070\n",
      "Epoch 181: val_loss did not improve from -45334.33984\n",
      " Epoch 181 - NLL: -8.7603 | Reg (weighted): 0.0700 | Total approx: -8.6903\n",
      "499/499 [==============================] - 55s 111ms/step - loss: -43034.7070 - val_loss: -45170.8008\n",
      "Epoch 182/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -42830.8945\n",
      "Epoch 182: val_loss did not improve from -45334.33984\n",
      " Epoch 182 - NLL: -8.7266 | Reg (weighted): 0.0704 | Total approx: -8.6563\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -42830.8945 - val_loss: -45172.6836\n",
      "Epoch 183/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43203.8203\n",
      "Epoch 183: val_loss did not improve from -45334.33984\n",
      " Epoch 183 - NLL: -8.7780 | Reg (weighted): 0.0694 | Total approx: -8.7086\n",
      "499/499 [==============================] - 56s 113ms/step - loss: -43203.8203 - val_loss: -44869.5430\n",
      "Epoch 184/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43221.8125\n",
      "Epoch 184: val_loss did not improve from -45334.33984\n",
      " Epoch 184 - NLL: -8.7740 | Reg (weighted): 0.0695 | Total approx: -8.7046\n",
      "499/499 [==============================] - 50s 101ms/step - loss: -43221.8125 - val_loss: -44657.9062\n",
      "Epoch 185/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43196.0859\n",
      "Epoch 185: val_loss did not improve from -45334.33984\n",
      " Epoch 185 - NLL: -8.7690 | Reg (weighted): 0.0691 | Total approx: -8.6999\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -43196.0859 - val_loss: -44646.0859\n",
      "Epoch 186/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43221.7734\n",
      "Epoch 186: val_loss did not improve from -45334.33984\n",
      " Epoch 186 - NLL: -8.7759 | Reg (weighted): 0.0690 | Total approx: -8.7070\n",
      "499/499 [==============================] - 48s 97ms/step - loss: -43221.7734 - val_loss: -44731.0430\n",
      "Epoch 187/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43180.1250\n",
      "Epoch 187: val_loss did not improve from -45334.33984\n",
      " Epoch 187 - NLL: -8.7820 | Reg (weighted): 0.0696 | Total approx: -8.7124\n",
      "499/499 [==============================] - 52s 103ms/step - loss: -43180.1250 - val_loss: -45103.5078\n",
      "Epoch 188/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43270.7539\n",
      "Epoch 188: val_loss did not improve from -45334.33984\n",
      " Epoch 188 - NLL: -8.7923 | Reg (weighted): 0.0694 | Total approx: -8.7228\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -43270.7539 - val_loss: -44962.1953\n",
      "Epoch 189/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43242.3359\n",
      "Epoch 189: val_loss did not improve from -45334.33984\n",
      " Epoch 189 - NLL: -8.7943 | Reg (weighted): 0.0688 | Total approx: -8.7255\n",
      "499/499 [==============================] - 54s 109ms/step - loss: -43242.3359 - val_loss: -45186.0547\n",
      "Epoch 190/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43275.3438\n",
      "Epoch 190: val_loss did not improve from -45334.33984\n",
      " Epoch 190 - NLL: -8.7968 | Reg (weighted): 0.0691 | Total approx: -8.7277\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -43275.3438 - val_loss: -45085.3633\n",
      "Epoch 191/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43252.9922\n",
      "Epoch 191: val_loss did not improve from -45334.33984\n",
      " Epoch 191 - NLL: -8.7938 | Reg (weighted): 0.0696 | Total approx: -8.7241\n",
      "499/499 [==============================] - 51s 103ms/step - loss: -43252.9922 - val_loss: -45091.1758\n",
      "Epoch 192/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43191.1016\n",
      "Epoch 192: val_loss did not improve from -45334.33984\n",
      " Epoch 192 - NLL: -8.7453 | Reg (weighted): 0.0699 | Total approx: -8.6754\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -43191.1016 - val_loss: -43932.2852\n",
      "Epoch 193/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43326.3711\n",
      "Epoch 193: val_loss did not improve from -45334.33984\n",
      " Epoch 193 - NLL: -8.7980 | Reg (weighted): 0.0693 | Total approx: -8.7287\n",
      "499/499 [==============================] - 51s 103ms/step - loss: -43326.3711 - val_loss: -44858.9258\n",
      "Epoch 194/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43254.5312\n",
      "Epoch 194: val_loss did not improve from -45334.33984\n",
      " Epoch 194 - NLL: -8.7697 | Reg (weighted): 0.0698 | Total approx: -8.6999\n",
      "499/499 [==============================] - 50s 100ms/step - loss: -43254.5312 - val_loss: -44352.6133\n",
      "Epoch 195/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43325.1484\n",
      "Epoch 195: val_loss did not improve from -45334.33984\n",
      " Epoch 195 - NLL: -8.8027 | Reg (weighted): 0.0693 | Total approx: -8.7335\n",
      "499/499 [==============================] - 50s 100ms/step - loss: -43325.1484 - val_loss: -45008.5703\n",
      "Epoch 196/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43262.7617\n",
      "Epoch 196: val_loss did not improve from -45334.33984\n",
      " Epoch 196 - NLL: -8.7945 | Reg (weighted): 0.0690 | Total approx: -8.7255\n",
      "499/499 [==============================] - 55s 109ms/step - loss: -43262.7617 - val_loss: -45081.6367\n",
      "Epoch 197/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43343.0664\n",
      "Epoch 197: val_loss did not improve from -45334.33984\n",
      " Epoch 197 - NLL: -8.8006 | Reg (weighted): 0.0693 | Total approx: -8.7314\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -43343.0664 - val_loss: -44854.9570\n",
      "Epoch 198/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43318.4023\n",
      "Epoch 198: val_loss did not improve from -45334.33984\n",
      " Epoch 198 - NLL: -8.7683 | Reg (weighted): 0.0694 | Total approx: -8.6988\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -43318.4023 - val_loss: -43998.4805\n",
      "Epoch 199/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43255.8789\n",
      "Epoch 199: val_loss did not improve from -45334.33984\n",
      " Epoch 199 - NLL: -8.7841 | Reg (weighted): 0.0697 | Total approx: -8.7144\n",
      "499/499 [==============================] - 56s 113ms/step - loss: -43255.8789 - val_loss: -44782.7344\n",
      "Epoch 200/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43373.0664\n",
      "Epoch 200: val_loss did not improve from -45334.33984\n",
      " Epoch 200 - NLL: -8.8200 | Reg (weighted): 0.0689 | Total approx: -8.7511\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -43373.0664 - val_loss: -45299.3711\n",
      "Epoch 201/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43383.9766\n",
      "Epoch 201: val_loss did not improve from -45334.33984\n",
      " Epoch 201 - NLL: -8.8178 | Reg (weighted): 0.0692 | Total approx: -8.7487\n",
      "499/499 [==============================] - 52s 103ms/step - loss: -43383.9766 - val_loss: -45169.9922\n",
      "Epoch 202/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43381.3867\n",
      "Epoch 202: val_loss did not improve from -45334.33984\n",
      " Epoch 202 - NLL: -8.8216 | Reg (weighted): 0.0692 | Total approx: -8.7524\n",
      "499/499 [==============================] - 58s 116ms/step - loss: -43381.3867 - val_loss: -45296.7930\n",
      "Epoch 203/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43373.9258\n",
      "Epoch 203: val_loss improved from -45334.33984 to -45382.14062, saving model to ./trained_models/model-0f649086-checkpoints/weights.203-t-43373.93-v-45382.14.hdf5\n",
      " Epoch 203 - NLL: -8.8230 | Reg (weighted): 0.0690 | Total approx: -8.7540\n",
      "499/499 [==============================] - 53s 107ms/step - loss: -43373.9258 - val_loss: -45382.1406\n",
      "Epoch 204/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43404.8125\n",
      "Epoch 204: val_loss did not improve from -45382.14062\n",
      " Epoch 204 - NLL: -8.8210 | Reg (weighted): 0.0689 | Total approx: -8.7521\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -43404.8125 - val_loss: -45170.3789\n",
      "Epoch 205/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43383.3945\n",
      "Epoch 205: val_loss improved from -45382.14062 to -45406.77734, saving model to ./trained_models/model-0f649086-checkpoints/weights.205-t-43383.39-v-45406.78.hdf5\n",
      " Epoch 205 - NLL: -8.8254 | Reg (weighted): 0.0690 | Total approx: -8.7564\n",
      "499/499 [==============================] - 53s 107ms/step - loss: -43383.3945 - val_loss: -45406.7773\n",
      "Epoch 206/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43405.0039\n",
      "Epoch 206: val_loss improved from -45406.77734 to -45417.35938, saving model to ./trained_models/model-0f649086-checkpoints/weights.206-t-43405.00-v-45417.36.hdf5\n",
      " Epoch 206 - NLL: -8.8292 | Reg (weighted): 0.0688 | Total approx: -8.7604\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -43405.0039 - val_loss: -45417.3594\n",
      "Epoch 207/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43333.4062\n",
      "Epoch 207: val_loss did not improve from -45417.35938\n",
      " Epoch 207 - NLL: -8.8143 | Reg (weighted): 0.0693 | Total approx: -8.7451\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -43333.4062 - val_loss: -45317.1641\n",
      "Epoch 208/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43418.5898\n",
      "Epoch 208: val_loss improved from -45417.35938 to -45423.59766, saving model to ./trained_models/model-0f649086-checkpoints/weights.208-t-43418.59-v-45423.60.hdf5\n",
      " Epoch 208 - NLL: -8.8315 | Reg (weighted): 0.0686 | Total approx: -8.7628\n",
      "499/499 [==============================] - 67s 133ms/step - loss: -43418.5898 - val_loss: -45423.5977\n",
      "Epoch 209/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43465.0352\n",
      "Epoch 209: val_loss did not improve from -45423.59766\n",
      " Epoch 209 - NLL: -8.8390 | Reg (weighted): 0.0687 | Total approx: -8.7703\n",
      "499/499 [==============================] - 49s 99ms/step - loss: -43465.0352 - val_loss: -45413.9102\n",
      "Epoch 210/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43436.0312\n",
      "Epoch 210: val_loss did not improve from -45423.59766\n",
      " Epoch 210 - NLL: -8.8333 | Reg (weighted): 0.0688 | Total approx: -8.7645\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -43436.0312 - val_loss: -45384.9883\n",
      "Epoch 211/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43465.4961\n",
      "Epoch 211: val_loss did not improve from -45423.59766\n",
      " Epoch 211 - NLL: -8.8321 | Reg (weighted): 0.0692 | Total approx: -8.7629\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -43465.4961 - val_loss: -45190.0000\n",
      "Epoch 212/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43476.3320\n",
      "Epoch 212: val_loss did not improve from -45423.59766\n",
      " Epoch 212 - NLL: -8.8262 | Reg (weighted): 0.0689 | Total approx: -8.7574\n",
      "499/499 [==============================] - 47s 95ms/step - loss: -43476.3320 - val_loss: -44967.2227\n",
      "Epoch 213/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43493.9414\n",
      "Epoch 213: val_loss did not improve from -45423.59766\n",
      " Epoch 213 - NLL: -8.8417 | Reg (weighted): 0.0686 | Total approx: -8.7730\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -43493.9414 - val_loss: -45351.5234\n",
      "Epoch 214/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43539.1172\n",
      "Epoch 214: val_loss did not improve from -45423.59766\n",
      " Epoch 214 - NLL: -8.8401 | Reg (weighted): 0.0690 | Total approx: -8.7711\n",
      "499/499 [==============================] - 59s 119ms/step - loss: -43539.1172 - val_loss: -45064.2891\n",
      "Epoch 215/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43542.9023\n",
      "Epoch 215: val_loss improved from -45423.59766 to -45449.53516, saving model to ./trained_models/model-0f649086-checkpoints/weights.215-t-43542.90-v-45449.54.hdf5\n",
      " Epoch 215 - NLL: -8.8530 | Reg (weighted): 0.0685 | Total approx: -8.7845\n",
      "499/499 [==============================] - 50s 101ms/step - loss: -43542.9023 - val_loss: -45449.5352\n",
      "Epoch 216/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43333.5430\n",
      "Epoch 216: val_loss did not improve from -45449.53516\n",
      " Epoch 216 - NLL: -8.7973 | Reg (weighted): 0.0700 | Total approx: -8.7273\n",
      "499/499 [==============================] - 55s 110ms/step - loss: -43333.5430 - val_loss: -44782.3164\n",
      "Epoch 217/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43625.5078\n",
      "Epoch 217: val_loss did not improve from -45449.53516\n",
      " Epoch 217 - NLL: -8.8614 | Reg (weighted): 0.0684 | Total approx: -8.7930\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -43625.5078 - val_loss: -45290.4414\n",
      "Epoch 218/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43582.3008\n",
      "Epoch 218: val_loss improved from -45449.53516 to -45535.82812, saving model to ./trained_models/model-0f649086-checkpoints/weights.218-t-43582.30-v-45535.83.hdf5\n",
      " Epoch 218 - NLL: -8.8625 | Reg (weighted): 0.0685 | Total approx: -8.7939\n",
      "499/499 [==============================] - 47s 94ms/step - loss: -43582.3008 - val_loss: -45535.8281\n",
      "Epoch 219/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43594.8242\n",
      "Epoch 219: val_loss did not improve from -45535.82812\n",
      " Epoch 219 - NLL: -8.8567 | Reg (weighted): 0.0686 | Total approx: -8.7882\n",
      "499/499 [==============================] - 49s 97ms/step - loss: -43594.8242 - val_loss: -45298.7891\n",
      "Epoch 220/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43641.0664\n",
      "Epoch 220: val_loss improved from -45535.82812 to -45601.54297, saving model to ./trained_models/model-0f649086-checkpoints/weights.220-t-43641.07-v-45601.54.hdf5\n",
      " Epoch 220 - NLL: -8.8741 | Reg (weighted): 0.0682 | Total approx: -8.8059\n",
      "499/499 [==============================] - 55s 110ms/step - loss: -43641.0664 - val_loss: -45601.5430\n",
      "Epoch 221/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43599.3789\n",
      "Epoch 221: val_loss did not improve from -45601.54297\n",
      " Epoch 221 - NLL: -8.8644 | Reg (weighted): 0.0683 | Total approx: -8.7961\n",
      "499/499 [==============================] - 51s 101ms/step - loss: -43599.3789 - val_loss: -45516.4375\n",
      "Epoch 222/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43651.7109\n",
      "Epoch 222: val_loss did not improve from -45601.54297\n",
      " Epoch 222 - NLL: -8.8707 | Reg (weighted): 0.0687 | Total approx: -8.8020\n",
      "499/499 [==============================] - 50s 101ms/step - loss: -43651.7109 - val_loss: -45430.9688\n",
      "Epoch 223/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43686.2773\n",
      "Epoch 223: val_loss did not improve from -45601.54297\n",
      " Epoch 223 - NLL: -8.8785 | Reg (weighted): 0.0688 | Total approx: -8.8097\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -43686.2773 - val_loss: -45487.9922\n",
      "Epoch 224/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43652.5352\n",
      "Epoch 224: val_loss did not improve from -45601.54297\n",
      " Epoch 224 - NLL: -8.8577 | Reg (weighted): 0.0684 | Total approx: -8.7893\n",
      "499/499 [==============================] - 47s 95ms/step - loss: -43652.5352 - val_loss: -45044.0352\n",
      "Epoch 225/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43660.1289\n",
      "Epoch 225: val_loss did not improve from -45601.54297\n",
      " Epoch 225 - NLL: -8.8362 | Reg (weighted): 0.0689 | Total approx: -8.7673\n",
      "499/499 [==============================] - 54s 109ms/step - loss: -43660.1289 - val_loss: -44340.5039\n",
      "Epoch 226/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43667.4844\n",
      "Epoch 226: val_loss did not improve from -45601.54297\n",
      " Epoch 226 - NLL: -8.8633 | Reg (weighted): 0.0685 | Total approx: -8.7948\n",
      "499/499 [==============================] - 56s 113ms/step - loss: -43667.4844 - val_loss: -45133.3125\n",
      "Epoch 227/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43682.8906\n",
      "Epoch 227: val_loss did not improve from -45601.54297\n",
      " Epoch 227 - NLL: -8.8455 | Reg (weighted): 0.0685 | Total approx: -8.7770\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -43682.8906 - val_loss: -44517.9883\n",
      "Epoch 228/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43708.7383\n",
      "Epoch 228: val_loss did not improve from -45601.54297\n",
      " Epoch 228 - NLL: -8.8817 | Reg (weighted): 0.0687 | Total approx: -8.8129\n",
      "499/499 [==============================] - 56s 112ms/step - loss: -43708.7383 - val_loss: -45472.4414\n",
      "Epoch 229/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43730.8555\n",
      "Epoch 229: val_loss did not improve from -45601.54297\n",
      " Epoch 229 - NLL: -8.8839 | Reg (weighted): 0.0683 | Total approx: -8.8155\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -43730.8555 - val_loss: -45438.2500\n",
      "Epoch 230/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43702.4609\n",
      "Epoch 230: val_loss did not improve from -45601.54297\n",
      " Epoch 230 - NLL: -8.8611 | Reg (weighted): 0.0680 | Total approx: -8.7931\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -43702.4609 - val_loss: -44905.6914\n",
      "Epoch 231/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43736.7969\n",
      "Epoch 231: val_loss did not improve from -45601.54297\n",
      " Epoch 231 - NLL: -8.8883 | Reg (weighted): 0.0684 | Total approx: -8.8199\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -43736.7969 - val_loss: -45540.9883\n",
      "Epoch 232/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43729.7305\n",
      "Epoch 232: val_loss did not improve from -45601.54297\n",
      " Epoch 232 - NLL: -8.8803 | Reg (weighted): 0.0683 | Total approx: -8.8120\n",
      "499/499 [==============================] - 57s 115ms/step - loss: -43729.7305 - val_loss: -45336.9492\n",
      "Epoch 233/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43784.9141\n",
      "Epoch 233: val_loss improved from -45601.54297 to -45624.03125, saving model to ./trained_models/model-0f649086-checkpoints/weights.233-t-43784.91-v-45624.03.hdf5\n",
      " Epoch 233 - NLL: -8.8987 | Reg (weighted): 0.0680 | Total approx: -8.8307\n",
      "499/499 [==============================] - 54s 109ms/step - loss: -43784.9141 - val_loss: -45624.0312\n",
      "Epoch 234/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43737.5508\n",
      "Epoch 234: val_loss did not improve from -45624.03125\n",
      " Epoch 234 - NLL: -8.8859 | Reg (weighted): 0.0683 | Total approx: -8.8176\n",
      "499/499 [==============================] - 56s 111ms/step - loss: -43737.5508 - val_loss: -45467.9922\n",
      "Epoch 235/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43719.0156\n",
      "Epoch 235: val_loss did not improve from -45624.03125\n",
      " Epoch 235 - NLL: -8.8834 | Reg (weighted): 0.0684 | Total approx: -8.8150\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -43719.0156 - val_loss: -45483.1992\n",
      "Epoch 236/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43814.3438\n",
      "Epoch 236: val_loss improved from -45624.03125 to -45677.64062, saving model to ./trained_models/model-0f649086-checkpoints/weights.236-t-43814.34-v-45677.64.hdf5\n",
      " Epoch 236 - NLL: -8.9057 | Reg (weighted): 0.0683 | Total approx: -8.8374\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -43814.3438 - val_loss: -45677.6406\n",
      "Epoch 237/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43773.0742\n",
      "Epoch 237: val_loss improved from -45677.64062 to -45695.11719, saving model to ./trained_models/model-0f649086-checkpoints/weights.237-t-43773.07-v-45695.12.hdf5\n",
      " Epoch 237 - NLL: -8.8994 | Reg (weighted): 0.0683 | Total approx: -8.8311\n",
      "499/499 [==============================] - 47s 95ms/step - loss: -43773.0742 - val_loss: -45695.1172\n",
      "Epoch 238/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43737.1758\n",
      "Epoch 238: val_loss did not improve from -45695.11719\n",
      " Epoch 238 - NLL: -8.8888 | Reg (weighted): 0.0681 | Total approx: -8.8207\n",
      "499/499 [==============================] - 59s 118ms/step - loss: -43737.1758 - val_loss: -45563.7734\n",
      "Epoch 239/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43828.3398\n",
      "Epoch 239: val_loss did not improve from -45695.11719\n",
      " Epoch 239 - NLL: -8.8957 | Reg (weighted): 0.0686 | Total approx: -8.8271\n",
      "499/499 [==============================] - 56s 111ms/step - loss: -43828.3398 - val_loss: -45296.5039\n",
      "Epoch 240/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43812.8008\n",
      "Epoch 240: val_loss did not improve from -45695.11719\n",
      " Epoch 240 - NLL: -8.8914 | Reg (weighted): 0.0686 | Total approx: -8.8229\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -43812.8008 - val_loss: -45246.5156\n",
      "Epoch 241/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43868.8945\n",
      "Epoch 241: val_loss did not improve from -45695.11719\n",
      " Epoch 241 - NLL: -8.8983 | Reg (weighted): 0.0686 | Total approx: -8.8298\n",
      "499/499 [==============================] - 49s 97ms/step - loss: -43868.8945 - val_loss: -45172.5156\n",
      "Epoch 242/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43806.7266\n",
      "Epoch 242: val_loss improved from -45695.11719 to -45764.00391, saving model to ./trained_models/model-0f649086-checkpoints/weights.242-t-43806.73-v-45764.00.hdf5\n",
      " Epoch 242 - NLL: -8.9074 | Reg (weighted): 0.0684 | Total approx: -8.8390\n",
      "499/499 [==============================] - 59s 118ms/step - loss: -43806.7266 - val_loss: -45764.0039\n",
      "Epoch 243/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43843.5742\n",
      "Epoch 243: val_loss did not improve from -45764.00391\n",
      " Epoch 243 - NLL: -8.9017 | Reg (weighted): 0.0683 | Total approx: -8.8333\n",
      "499/499 [==============================] - 50s 100ms/step - loss: -43843.5742 - val_loss: -45407.8164\n",
      "Epoch 244/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43846.3320\n",
      "Epoch 244: val_loss did not improve from -45764.00391\n",
      " Epoch 244 - NLL: -8.9129 | Reg (weighted): 0.0682 | Total approx: -8.8448\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -43846.3320 - val_loss: -45737.9766\n",
      "Epoch 245/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43864.0742\n",
      "Epoch 245: val_loss did not improve from -45764.00391\n",
      " Epoch 245 - NLL: -8.9110 | Reg (weighted): 0.0676 | Total approx: -8.8434\n",
      "499/499 [==============================] - 51s 103ms/step - loss: -43864.0742 - val_loss: -45606.3398\n",
      "Epoch 246/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43861.5547\n",
      "Epoch 246: val_loss did not improve from -45764.00391\n",
      " Epoch 246 - NLL: -8.9006 | Reg (weighted): 0.0680 | Total approx: -8.8326\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -43861.5547 - val_loss: -45295.2344\n",
      "Epoch 247/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43850.6523\n",
      "Epoch 247: val_loss did not improve from -45764.00391\n",
      " Epoch 247 - NLL: -8.9130 | Reg (weighted): 0.0681 | Total approx: -8.8450\n",
      "499/499 [==============================] - 50s 100ms/step - loss: -43850.6523 - val_loss: -45723.4531\n",
      "Epoch 248/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43893.1914\n",
      "Epoch 248: val_loss did not improve from -45764.00391\n",
      " Epoch 248 - NLL: -8.9101 | Reg (weighted): 0.0680 | Total approx: -8.8422\n",
      "499/499 [==============================] - 47s 95ms/step - loss: -43893.1914 - val_loss: -45424.4961\n",
      "Epoch 249/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43895.3789\n",
      "Epoch 249: val_loss did not improve from -45764.00391\n",
      " Epoch 249 - NLL: -8.9073 | Reg (weighted): 0.0683 | Total approx: -8.8390\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -43895.3789 - val_loss: -45316.7969\n",
      "Epoch 250/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43899.9961\n",
      "Epoch 250: val_loss did not improve from -45764.00391\n",
      " Epoch 250 - NLL: -8.9149 | Reg (weighted): 0.0682 | Total approx: -8.8468\n",
      "499/499 [==============================] - 51s 103ms/step - loss: -43899.9961 - val_loss: -45527.7969\n",
      "Epoch 251/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43884.0703\n",
      "Epoch 251: val_loss did not improve from -45764.00391\n",
      " Epoch 251 - NLL: -8.9026 | Reg (weighted): 0.0680 | Total approx: -8.8345\n",
      "499/499 [==============================] - 56s 112ms/step - loss: -43884.0703 - val_loss: -45239.2812\n",
      "Epoch 252/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43940.0703\n",
      "Epoch 252: val_loss improved from -45764.00391 to -45815.05469, saving model to ./trained_models/model-0f649086-checkpoints/weights.252-t-43940.07-v-45815.05.hdf5\n",
      " Epoch 252 - NLL: -8.9308 | Reg (weighted): 0.0678 | Total approx: -8.8630\n",
      "499/499 [==============================] - 60s 120ms/step - loss: -43940.0703 - val_loss: -45815.0547\n",
      "Epoch 253/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43883.2734\n",
      "Epoch 253: val_loss did not improve from -45815.05469\n",
      " Epoch 253 - NLL: -8.9141 | Reg (weighted): 0.0682 | Total approx: -8.8459\n",
      "499/499 [==============================] - 49s 99ms/step - loss: -43883.2734 - val_loss: -45586.8867\n",
      "Epoch 254/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43928.0977\n",
      "Epoch 254: val_loss did not improve from -45815.05469\n",
      " Epoch 254 - NLL: -8.9243 | Reg (weighted): 0.0679 | Total approx: -8.8563\n",
      "499/499 [==============================] - 47s 95ms/step - loss: -43928.0977 - val_loss: -45675.1484\n",
      "Epoch 255/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43913.9844\n",
      "Epoch 255: val_loss did not improve from -45815.05469\n",
      " Epoch 255 - NLL: -8.9224 | Reg (weighted): 0.0678 | Total approx: -8.8546\n",
      "499/499 [==============================] - 54s 107ms/step - loss: -43913.9844 - val_loss: -45692.9766\n",
      "Epoch 256/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43944.2461\n",
      "Epoch 256: val_loss did not improve from -45815.05469\n",
      " Epoch 256 - NLL: -8.9159 | Reg (weighted): 0.0682 | Total approx: -8.8477\n",
      "499/499 [==============================] - 50s 100ms/step - loss: -43944.2461 - val_loss: -45332.7422\n",
      "Epoch 257/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43956.5859\n",
      "Epoch 257: val_loss did not improve from -45815.05469\n",
      " Epoch 257 - NLL: -8.9120 | Reg (weighted): 0.0683 | Total approx: -8.8437\n",
      "499/499 [==============================] - 57s 113ms/step - loss: -43956.5859 - val_loss: -45151.4570\n",
      "Epoch 258/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43901.4336\n",
      "Epoch 258: val_loss did not improve from -45815.05469\n",
      " Epoch 258 - NLL: -8.9146 | Reg (weighted): 0.0682 | Total approx: -8.8463\n",
      "499/499 [==============================] - 55s 110ms/step - loss: -43901.4336 - val_loss: -45507.6680\n",
      "Epoch 259/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44011.1914\n",
      "Epoch 259: val_loss improved from -45815.05469 to -45868.73828, saving model to ./trained_models/model-0f649086-checkpoints/weights.259-t-44011.19-v-45868.74.hdf5\n",
      " Epoch 259 - NLL: -8.9445 | Reg (weighted): 0.0679 | Total approx: -8.8766\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44011.1914 - val_loss: -45868.7383\n",
      "Epoch 260/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43772.5742\n",
      "Epoch 260: val_loss improved from -45868.73828 to -46166.00000, saving model to ./trained_models/model-0f649086-checkpoints/weights.260-t-43772.57-v-46166.00.hdf5\n",
      " Epoch 260 - NLL: -8.9147 | Reg (weighted): 0.0682 | Total approx: -8.8466\n",
      "499/499 [==============================] - 47s 94ms/step - loss: -43772.5742 - val_loss: -46166.0000\n",
      "Epoch 261/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43996.1016\n",
      "Epoch 261: val_loss did not improve from -46166.00000\n",
      " Epoch 261 - NLL: -8.9343 | Reg (weighted): 0.0676 | Total approx: -8.8668\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -43996.1016 - val_loss: -45646.5586\n",
      "Epoch 262/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43967.7109\n",
      "Epoch 262: val_loss did not improve from -46166.00000\n",
      " Epoch 262 - NLL: -8.9348 | Reg (weighted): 0.0678 | Total approx: -8.8670\n",
      "499/499 [==============================] - 46s 92ms/step - loss: -43967.7109 - val_loss: -45798.1523\n",
      "Epoch 263/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43997.9961\n",
      "Epoch 263: val_loss did not improve from -46166.00000\n",
      " Epoch 263 - NLL: -8.9285 | Reg (weighted): 0.0680 | Total approx: -8.8605\n",
      "499/499 [==============================] - 59s 118ms/step - loss: -43997.9961 - val_loss: -45449.0664\n",
      "Epoch 264/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44007.8906\n",
      "Epoch 264: val_loss did not improve from -46166.00000\n",
      " Epoch 264 - NLL: -8.9399 | Reg (weighted): 0.0676 | Total approx: -8.8722\n",
      "499/499 [==============================] - 55s 110ms/step - loss: -44007.8906 - val_loss: -45752.0508\n",
      "Epoch 265/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43950.0547\n",
      "Epoch 265: val_loss did not improve from -46166.00000\n",
      " Epoch 265 - NLL: -8.9365 | Reg (weighted): 0.0680 | Total approx: -8.8685\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -43950.0547 - val_loss: -45931.5508\n",
      "Epoch 266/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44022.9766\n",
      "Epoch 266: val_loss did not improve from -46166.00000\n",
      " Epoch 266 - NLL: -8.9453 | Reg (weighted): 0.0677 | Total approx: -8.8776\n",
      "499/499 [==============================] - 47s 94ms/step - loss: -44022.9766 - val_loss: -45838.2695\n",
      "Epoch 267/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44021.3086\n",
      "Epoch 267: val_loss did not improve from -46166.00000\n",
      " Epoch 267 - NLL: -8.8785 | Reg (weighted): 0.0686 | Total approx: -8.8099\n",
      "499/499 [==============================] - 53s 105ms/step - loss: -44021.3086 - val_loss: -43804.8672\n",
      "Epoch 268/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44010.0117\n",
      "Epoch 268: val_loss did not improve from -46166.00000\n",
      " Epoch 268 - NLL: -8.9473 | Reg (weighted): 0.0680 | Total approx: -8.8793\n",
      "499/499 [==============================] - 47s 95ms/step - loss: -44010.0117 - val_loss: -45955.9727\n",
      "Epoch 269/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43981.0625\n",
      "Epoch 269: val_loss did not improve from -46166.00000\n",
      " Epoch 269 - NLL: -8.9294 | Reg (weighted): 0.0677 | Total approx: -8.8617\n",
      "499/499 [==============================] - 55s 111ms/step - loss: -43981.0625 - val_loss: -45571.3594\n",
      "Epoch 270/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44050.1641\n",
      "Epoch 270: val_loss did not improve from -46166.00000\n",
      " Epoch 270 - NLL: -8.9364 | Reg (weighted): 0.0676 | Total approx: -8.8688\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -44050.1641 - val_loss: -45435.6016\n",
      "Epoch 271/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44082.5664\n",
      "Epoch 271: val_loss did not improve from -46166.00000\n",
      " Epoch 271 - NLL: -8.9552 | Reg (weighted): 0.0675 | Total approx: -8.8877\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -44082.5664 - val_loss: -45841.8789\n",
      "Epoch 272/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43873.1172\n",
      "Epoch 272: val_loss did not improve from -46166.00000\n",
      " Epoch 272 - NLL: -8.9211 | Reg (weighted): 0.0681 | Total approx: -8.8530\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -43873.1172 - val_loss: -45853.6641\n",
      "Epoch 273/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44092.6328\n",
      "Epoch 273: val_loss did not improve from -46166.00000\n",
      " Epoch 273 - NLL: -8.9517 | Reg (weighted): 0.0676 | Total approx: -8.8841\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -44092.6328 - val_loss: -45682.8945\n",
      "Epoch 274/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44063.3125\n",
      "Epoch 274: val_loss did not improve from -46166.00000\n",
      " Epoch 274 - NLL: -8.9466 | Reg (weighted): 0.0680 | Total approx: -8.8786\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44063.3125 - val_loss: -45664.8750\n",
      "Epoch 275/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -43984.5938\n",
      "Epoch 275: val_loss did not improve from -46166.00000\n",
      " Epoch 275 - NLL: -8.9435 | Reg (weighted): 0.0678 | Total approx: -8.8757\n",
      "499/499 [==============================] - 52s 105ms/step - loss: -43984.5938 - val_loss: -45975.5430\n",
      "Epoch 276/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44116.8164\n",
      "Epoch 276: val_loss did not improve from -46166.00000\n",
      " Epoch 276 - NLL: -8.9487 | Reg (weighted): 0.0676 | Total approx: -8.8811\n",
      "499/499 [==============================] - 57s 114ms/step - loss: -44116.8164 - val_loss: -45470.9062\n",
      "Epoch 277/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44081.6680\n",
      "Epoch 277: val_loss did not improve from -46166.00000\n",
      " Epoch 277 - NLL: -8.9472 | Reg (weighted): 0.0680 | Total approx: -8.8792\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -44081.6680 - val_loss: -45590.6797\n",
      "Epoch 278/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44112.0898\n",
      "Epoch 278: val_loss did not improve from -46166.00000\n",
      " Epoch 278 - NLL: -8.9460 | Reg (weighted): 0.0677 | Total approx: -8.8783\n",
      "499/499 [==============================] - 52s 105ms/step - loss: -44112.0898 - val_loss: -45410.9922\n",
      "Epoch 279/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44057.1250\n",
      "Epoch 279: val_loss did not improve from -46166.00000\n",
      " Epoch 279 - NLL: -8.9408 | Reg (weighted): 0.0679 | Total approx: -8.8729\n",
      "499/499 [==============================] - 53s 105ms/step - loss: -44057.1250 - val_loss: -45524.3711\n",
      "Epoch 280/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44136.6016\n",
      "Epoch 280: val_loss did not improve from -46166.00000\n",
      " Epoch 280 - NLL: -8.9567 | Reg (weighted): 0.0673 | Total approx: -8.8894\n",
      "499/499 [==============================] - 47s 93ms/step - loss: -44136.6016 - val_loss: -45620.0625\n",
      "Epoch 281/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44137.2188\n",
      "Epoch 281: val_loss did not improve from -46166.00000\n",
      " Epoch 281 - NLL: -8.9625 | Reg (weighted): 0.0675 | Total approx: -8.8949\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -44137.2188 - val_loss: -45785.2812\n",
      "Epoch 282/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44105.7109\n",
      "Epoch 282: val_loss did not improve from -46166.00000\n",
      " Epoch 282 - NLL: -8.9572 | Reg (weighted): 0.0678 | Total approx: -8.8894\n",
      "499/499 [==============================] - 57s 114ms/step - loss: -44105.7109 - val_loss: -45776.0742\n",
      "Epoch 283/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44121.6250\n",
      "Epoch 283: val_loss did not improve from -46166.00000\n",
      " Epoch 283 - NLL: -8.9589 | Reg (weighted): 0.0675 | Total approx: -8.8914\n",
      "499/499 [==============================] - 50s 101ms/step - loss: -44121.6250 - val_loss: -45757.1250\n",
      "Epoch 284/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44131.3398\n",
      "Epoch 284: val_loss did not improve from -46166.00000\n",
      " Epoch 284 - NLL: -8.9607 | Reg (weighted): 0.0670 | Total approx: -8.8937\n",
      "499/499 [==============================] - 50s 101ms/step - loss: -44131.3398 - val_loss: -45778.5039\n",
      "Epoch 285/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44062.8477\n",
      "Epoch 285: val_loss did not improve from -46166.00000\n",
      " Epoch 285 - NLL: -8.9516 | Reg (weighted): 0.0676 | Total approx: -8.8839\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -44062.8477 - val_loss: -45828.7422\n",
      "Epoch 286/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44158.4297\n",
      "Epoch 286: val_loss did not improve from -46166.00000\n",
      " Epoch 286 - NLL: -8.9706 | Reg (weighted): 0.0673 | Total approx: -8.9033\n",
      "499/499 [==============================] - 47s 94ms/step - loss: -44158.4297 - val_loss: -45931.5430\n",
      "Epoch 287/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44110.0156\n",
      "Epoch 287: val_loss did not improve from -46166.00000\n",
      " Epoch 287 - NLL: -8.9603 | Reg (weighted): 0.0677 | Total approx: -8.8926\n",
      "499/499 [==============================] - 48s 97ms/step - loss: -44110.0156 - val_loss: -45850.6328\n",
      "Epoch 288/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44189.7383\n",
      "Epoch 288: val_loss did not improve from -46166.00000\n",
      " Epoch 288 - NLL: -8.9712 | Reg (weighted): 0.0673 | Total approx: -8.9038\n",
      "499/499 [==============================] - 73s 145ms/step - loss: -44189.7383 - val_loss: -45788.9336\n",
      "Epoch 289/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44037.2852\n",
      "Epoch 289: val_loss did not improve from -46166.00000\n",
      " Epoch 289 - NLL: -8.9355 | Reg (weighted): 0.0678 | Total approx: -8.8676\n",
      "499/499 [==============================] - 50s 100ms/step - loss: -44037.2852 - val_loss: -45464.3086\n",
      "Epoch 290/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44191.7773\n",
      "Epoch 290: val_loss did not improve from -46166.00000\n",
      " Epoch 290 - NLL: -8.9663 | Reg (weighted): 0.0676 | Total approx: -8.8987\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -44191.7773 - val_loss: -45622.9297\n",
      "Epoch 291/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44153.9570\n",
      "Epoch 291: val_loss did not improve from -46166.00000\n",
      " Epoch 291 - NLL: -8.9511 | Reg (weighted): 0.0672 | Total approx: -8.8839\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44153.9570 - val_loss: -45368.8711\n",
      "Epoch 292/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44190.5391\n",
      "Epoch 292: val_loss did not improve from -46166.00000\n",
      " Epoch 292 - NLL: -8.9646 | Reg (weighted): 0.0672 | Total approx: -8.8974\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -44190.5391 - val_loss: -45590.9570\n",
      "Epoch 293/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44182.6328\n",
      "Epoch 293: val_loss did not improve from -46166.00000\n",
      " Epoch 293 - NLL: -8.9721 | Reg (weighted): 0.0673 | Total approx: -8.9048\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44182.6328 - val_loss: -45852.6680\n",
      "Epoch 294/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44223.4805\n",
      "Epoch 294: val_loss did not improve from -46166.00000\n",
      " Epoch 294 - NLL: -8.9678 | Reg (weighted): 0.0674 | Total approx: -8.9004\n",
      "499/499 [==============================] - 59s 119ms/step - loss: -44223.4805 - val_loss: -45516.3594\n",
      "Epoch 295/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44185.5898\n",
      "Epoch 295: val_loss did not improve from -46166.00000\n",
      " Epoch 295 - NLL: -8.9714 | Reg (weighted): 0.0669 | Total approx: -8.9045\n",
      "499/499 [==============================] - 49s 97ms/step - loss: -44185.5898 - val_loss: -45829.0469\n",
      "Epoch 296/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44201.1992\n",
      "Epoch 296: val_loss did not improve from -46166.00000\n",
      " Epoch 296 - NLL: -8.9667 | Reg (weighted): 0.0675 | Total approx: -8.8992\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -44201.1992 - val_loss: -45591.5273\n",
      "Epoch 297/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44205.7305\n",
      "Epoch 297: val_loss did not improve from -46166.00000\n",
      " Epoch 297 - NLL: -8.9766 | Reg (weighted): 0.0674 | Total approx: -8.9092\n",
      "499/499 [==============================] - 49s 97ms/step - loss: -44205.7305 - val_loss: -45869.4414\n",
      "Epoch 298/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44183.6016\n",
      "Epoch 298: val_loss did not improve from -46166.00000\n",
      " Epoch 298 - NLL: -8.9705 | Reg (weighted): 0.0671 | Total approx: -8.9034\n",
      "499/499 [==============================] - 47s 95ms/step - loss: -44183.6016 - val_loss: -45806.6406\n",
      "Epoch 299/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44211.0664\n",
      "Epoch 299: val_loss did not improve from -46166.00000\n",
      " Epoch 299 - NLL: -8.9681 | Reg (weighted): 0.0674 | Total approx: -8.9007\n",
      "499/499 [==============================] - 50s 99ms/step - loss: -44211.0664 - val_loss: -45586.9688\n",
      "Epoch 300/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44235.6953\n",
      "Epoch 300: val_loss did not improve from -46166.00000\n",
      " Epoch 300 - NLL: -8.9897 | Reg (weighted): 0.0671 | Total approx: -8.9226\n",
      "499/499 [==============================] - 58s 117ms/step - loss: -44235.6953 - val_loss: -46123.2305\n",
      "Epoch 301/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44187.9414\n",
      "Epoch 301: val_loss did not improve from -46166.00000\n",
      " Epoch 301 - NLL: -8.9087 | Reg (weighted): 0.0680 | Total approx: -8.8407\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -44187.9414 - val_loss: -43893.4258\n",
      "Epoch 302/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44178.7930\n",
      "Epoch 302: val_loss improved from -46166.00000 to -46224.30078, saving model to ./trained_models/model-0f649086-checkpoints/weights.302-t-44178.79-v-46224.30.hdf5\n",
      " Epoch 302 - NLL: -8.9839 | Reg (weighted): 0.0674 | Total approx: -8.9164\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -44178.7930 - val_loss: -46224.3008\n",
      "Epoch 303/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44213.3320\n",
      "Epoch 303: val_loss did not improve from -46224.30078\n",
      " Epoch 303 - NLL: -8.9862 | Reg (weighted): 0.0671 | Total approx: -8.9191\n",
      "499/499 [==============================] - 65s 129ms/step - loss: -44213.3320 - val_loss: -46128.7227\n",
      "Epoch 304/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44266.7188\n",
      "Epoch 304: val_loss did not improve from -46224.30078\n",
      " Epoch 304 - NLL: -8.9843 | Reg (weighted): 0.0671 | Total approx: -8.9172\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44266.7188 - val_loss: -45805.4805\n",
      "Epoch 305/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44236.1055\n",
      "Epoch 305: val_loss did not improve from -46224.30078\n",
      " Epoch 305 - NLL: -8.9598 | Reg (weighted): 0.0671 | Total approx: -8.8928\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44236.1055 - val_loss: -45221.7148\n",
      "Epoch 306/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44217.5625\n",
      "Epoch 306: val_loss did not improve from -46224.30078\n",
      " Epoch 306 - NLL: -8.9787 | Reg (weighted): 0.0670 | Total approx: -8.9117\n",
      "499/499 [==============================] - 65s 129ms/step - loss: -44217.5625 - val_loss: -45884.9570\n",
      "Epoch 307/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44272.2305\n",
      "Epoch 307: val_loss did not improve from -46224.30078\n",
      " Epoch 307 - NLL: -8.9946 | Reg (weighted): 0.0673 | Total approx: -8.9273\n",
      "499/499 [==============================] - 55s 109ms/step - loss: -44272.2305 - val_loss: -46080.9961\n",
      "Epoch 308/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44291.3008\n",
      "Epoch 308: val_loss did not improve from -46224.30078\n",
      " Epoch 308 - NLL: -8.9948 | Reg (weighted): 0.0672 | Total approx: -8.9276\n",
      "499/499 [==============================] - 51s 103ms/step - loss: -44291.3008 - val_loss: -45993.2344\n",
      "Epoch 309/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44248.3672\n",
      "Epoch 309: val_loss did not improve from -46224.30078\n",
      " Epoch 309 - NLL: -8.9822 | Reg (weighted): 0.0669 | Total approx: -8.9153\n",
      "499/499 [==============================] - 54s 107ms/step - loss: -44248.3672 - val_loss: -45837.7539\n",
      "Epoch 310/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44296.4766\n",
      "Epoch 310: val_loss did not improve from -46224.30078\n",
      " Epoch 310 - NLL: -8.9897 | Reg (weighted): 0.0672 | Total approx: -8.9225\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -44296.4766 - val_loss: -45812.7422\n",
      "Epoch 311/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44220.5156\n",
      "Epoch 311: val_loss did not improve from -46224.30078\n",
      " Epoch 311 - NLL: -8.9764 | Reg (weighted): 0.0671 | Total approx: -8.9093\n",
      "499/499 [==============================] - 57s 113ms/step - loss: -44220.5156 - val_loss: -45797.3477\n",
      "Epoch 312/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44301.2695\n",
      "Epoch 312: val_loss did not improve from -46224.30078\n",
      " Epoch 312 - NLL: -9.0006 | Reg (weighted): 0.0668 | Total approx: -8.9338\n",
      "499/499 [==============================] - 60s 121ms/step - loss: -44301.2695 - val_loss: -46129.5156\n",
      "Epoch 313/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44290.5430\n",
      "Epoch 313: val_loss did not improve from -46224.30078\n",
      " Epoch 313 - NLL: -8.9996 | Reg (weighted): 0.0670 | Total approx: -8.9325\n",
      "499/499 [==============================] - 56s 112ms/step - loss: -44290.5430 - val_loss: -46146.3672\n",
      "Epoch 314/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44304.4102\n",
      "Epoch 314: val_loss did not improve from -46224.30078\n",
      " Epoch 314 - NLL: -8.9942 | Reg (weighted): 0.0668 | Total approx: -8.9274\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -44304.4102 - val_loss: -45922.4648\n",
      "Epoch 315/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44237.8320\n",
      "Epoch 315: val_loss did not improve from -46224.30078\n",
      " Epoch 315 - NLL: -8.9879 | Reg (weighted): 0.0669 | Total approx: -8.9210\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44237.8320 - val_loss: -46062.6367\n",
      "Epoch 316/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44306.2734\n",
      "Epoch 316: val_loss did not improve from -46224.30078\n",
      " Epoch 316 - NLL: -8.9914 | Reg (weighted): 0.0673 | Total approx: -8.9241\n",
      "499/499 [==============================] - 49s 97ms/step - loss: -44306.2734 - val_loss: -45812.2852\n",
      "Epoch 317/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44292.8398\n",
      "Epoch 317: val_loss did not improve from -46224.30078\n",
      " Epoch 317 - NLL: -8.9886 | Reg (weighted): 0.0668 | Total approx: -8.9218\n",
      "499/499 [==============================] - 48s 97ms/step - loss: -44292.8398 - val_loss: -45811.5625\n",
      "Epoch 318/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44329.6367\n",
      "Epoch 318: val_loss did not improve from -46224.30078\n",
      " Epoch 318 - NLL: -9.0050 | Reg (weighted): 0.0667 | Total approx: -8.9383\n",
      "499/499 [==============================] - 59s 119ms/step - loss: -44329.6367 - val_loss: -46122.0000\n",
      "Epoch 319/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44166.0273\n",
      "Epoch 319: val_loss did not improve from -46224.30078\n",
      " Epoch 319 - NLL: -8.9657 | Reg (weighted): 0.0674 | Total approx: -8.8982\n",
      "499/499 [==============================] - 55s 111ms/step - loss: -44166.0273 - val_loss: -45741.2383\n",
      "Epoch 320/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44324.0000\n",
      "Epoch 320: val_loss improved from -46224.30078 to -46355.91016, saving model to ./trained_models/model-0f649086-checkpoints/weights.320-t-44324.00-v-46355.91.hdf5\n",
      " Epoch 320 - NLL: -9.0120 | Reg (weighted): 0.0669 | Total approx: -8.9451\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44324.0000 - val_loss: -46355.9102\n",
      "Epoch 321/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44282.7344\n",
      "Epoch 321: val_loss did not improve from -46355.91016\n",
      " Epoch 321 - NLL: -8.9915 | Reg (weighted): 0.0669 | Total approx: -8.9247\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44282.7344 - val_loss: -45947.6055\n",
      "Epoch 322/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44320.6133\n",
      "Epoch 322: val_loss did not improve from -46355.91016\n",
      " Epoch 322 - NLL: -8.9976 | Reg (weighted): 0.0672 | Total approx: -8.9303\n",
      "499/499 [==============================] - 47s 95ms/step - loss: -44320.6133 - val_loss: -45927.9844\n",
      "Epoch 323/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44341.8125\n",
      "Epoch 323: val_loss did not improve from -46355.91016\n",
      " Epoch 323 - NLL: -8.9984 | Reg (weighted): 0.0671 | Total approx: -8.9313\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -44341.8125 - val_loss: -45850.2266\n",
      "Epoch 324/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44326.0547\n",
      "Epoch 324: val_loss did not improve from -46355.91016\n",
      " Epoch 324 - NLL: -8.9897 | Reg (weighted): 0.0668 | Total approx: -8.9230\n",
      "499/499 [==============================] - 54s 107ms/step - loss: -44326.0547 - val_loss: -45678.0000\n",
      "Epoch 325/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44334.6719\n",
      "Epoch 325: val_loss improved from -46355.91016 to -46386.61328, saving model to ./trained_models/model-0f649086-checkpoints/weights.325-t-44334.67-v-46386.61.hdf5\n",
      " Epoch 325 - NLL: -9.0145 | Reg (weighted): 0.0666 | Total approx: -8.9479\n",
      "499/499 [==============================] - 57s 114ms/step - loss: -44334.6719 - val_loss: -46386.6133\n",
      "Epoch 326/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44330.4336\n",
      "Epoch 326: val_loss did not improve from -46386.61328\n",
      " Epoch 326 - NLL: -8.9988 | Reg (weighted): 0.0671 | Total approx: -8.9317\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -44330.4336 - val_loss: -45921.4805\n",
      "Epoch 327/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44358.5469\n",
      "Epoch 327: val_loss did not improve from -46386.61328\n",
      " Epoch 327 - NLL: -9.0058 | Reg (weighted): 0.0671 | Total approx: -8.9387\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -44358.5469 - val_loss: -45990.7344\n",
      "Epoch 328/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44351.3398\n",
      "Epoch 328: val_loss did not improve from -46386.61328\n",
      " Epoch 328 - NLL: -9.0093 | Reg (weighted): 0.0673 | Total approx: -8.9420\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -44351.3398 - val_loss: -46125.7930\n",
      "Epoch 329/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44369.8555\n",
      "Epoch 329: val_loss did not improve from -46386.61328\n",
      " Epoch 329 - NLL: -9.0114 | Reg (weighted): 0.0669 | Total approx: -8.9445\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -44369.8555 - val_loss: -46107.7852\n",
      "Epoch 330/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44377.3711\n",
      "Epoch 330: val_loss did not improve from -46386.61328\n",
      " Epoch 330 - NLL: -9.0055 | Reg (weighted): 0.0669 | Total approx: -8.9385\n",
      "499/499 [==============================] - 49s 97ms/step - loss: -44377.3711 - val_loss: -45889.1016\n",
      "Epoch 331/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44371.7227\n",
      "Epoch 331: val_loss did not improve from -46386.61328\n",
      " Epoch 331 - NLL: -9.0052 | Reg (weighted): 0.0669 | Total approx: -8.9384\n",
      "499/499 [==============================] - 59s 119ms/step - loss: -44371.7227 - val_loss: -45912.7227\n",
      "Epoch 332/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44367.8086\n",
      "Epoch 332: val_loss did not improve from -46386.61328\n",
      " Epoch 332 - NLL: -9.0032 | Reg (weighted): 0.0670 | Total approx: -8.9362\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -44367.8086 - val_loss: -45866.6133\n",
      "Epoch 333/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44377.1211\n",
      "Epoch 333: val_loss did not improve from -46386.61328\n",
      " Epoch 333 - NLL: -9.0126 | Reg (weighted): 0.0666 | Total approx: -8.9460\n",
      "499/499 [==============================] - 48s 97ms/step - loss: -44377.1211 - val_loss: -46116.0391\n",
      "Epoch 334/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44386.0000\n",
      "Epoch 334: val_loss did not improve from -46386.61328\n",
      " Epoch 334 - NLL: -8.9984 | Reg (weighted): 0.0668 | Total approx: -8.9316\n",
      "499/499 [==============================] - 48s 97ms/step - loss: -44386.0000 - val_loss: -45636.2070\n",
      "Epoch 335/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44311.5117\n",
      "Epoch 335: val_loss did not improve from -46386.61328\n",
      " Epoch 335 - NLL: -9.0026 | Reg (weighted): 0.0669 | Total approx: -8.9358\n",
      "499/499 [==============================] - 50s 99ms/step - loss: -44311.5117 - val_loss: -46137.5742\n",
      "Epoch 336/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44419.6016\n",
      "Epoch 336: val_loss did not improve from -46386.61328\n",
      " Epoch 336 - NLL: -9.0173 | Reg (weighted): 0.0670 | Total approx: -8.9503\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -44419.6016 - val_loss: -46032.5742\n",
      "Epoch 337/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44406.9453\n",
      "Epoch 337: val_loss did not improve from -46386.61328\n",
      " Epoch 337 - NLL: -9.0003 | Reg (weighted): 0.0668 | Total approx: -8.9334\n",
      "499/499 [==============================] - 58s 116ms/step - loss: -44406.9453 - val_loss: -45586.2773\n",
      "Epoch 338/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44407.3164\n",
      "Epoch 338: val_loss did not improve from -46386.61328\n",
      " Epoch 338 - NLL: -9.0245 | Reg (weighted): 0.0668 | Total approx: -8.9577\n",
      "499/499 [==============================] - 50s 100ms/step - loss: -44407.3164 - val_loss: -46316.8516\n",
      "Epoch 339/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44370.3164\n",
      "Epoch 339: val_loss did not improve from -46386.61328\n",
      " Epoch 339 - NLL: -9.0130 | Reg (weighted): 0.0667 | Total approx: -8.9462\n",
      "499/499 [==============================] - 51s 103ms/step - loss: -44370.3164 - val_loss: -46157.2617\n",
      "Epoch 340/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44430.0586\n",
      "Epoch 340: val_loss did not improve from -46386.61328\n",
      " Epoch 340 - NLL: -9.0126 | Reg (weighted): 0.0667 | Total approx: -8.9459\n",
      "499/499 [==============================] - 47s 95ms/step - loss: -44430.0586 - val_loss: -45845.5508\n",
      "Epoch 341/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44395.7969\n",
      "Epoch 341: val_loss did not improve from -46386.61328\n",
      " Epoch 341 - NLL: -9.0167 | Reg (weighted): 0.0666 | Total approx: -8.9501\n",
      "499/499 [==============================] - 48s 97ms/step - loss: -44395.7969 - val_loss: -46146.0078\n",
      "Epoch 342/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44399.3672\n",
      "Epoch 342: val_loss improved from -46386.61328 to -46425.18750, saving model to ./trained_models/model-0f649086-checkpoints/weights.342-t-44399.37-v-46425.19.hdf5\n",
      " Epoch 342 - NLL: -9.0265 | Reg (weighted): 0.0665 | Total approx: -8.9600\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -44399.3672 - val_loss: -46425.1875\n",
      "Epoch 343/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44422.3125\n",
      "Epoch 343: val_loss did not improve from -46425.18750\n",
      " Epoch 343 - NLL: -9.0150 | Reg (weighted): 0.0670 | Total approx: -8.9480\n",
      "499/499 [==============================] - 63s 125ms/step - loss: -44422.3125 - val_loss: -45948.2891\n",
      "Epoch 344/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44380.6367\n",
      "Epoch 344: val_loss did not improve from -46425.18750\n",
      " Epoch 344 - NLL: -9.0194 | Reg (weighted): 0.0670 | Total approx: -8.9523\n",
      "499/499 [==============================] - 52s 105ms/step - loss: -44380.6367 - val_loss: -46290.2031\n",
      "Epoch 345/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44378.0391\n",
      "Epoch 345: val_loss did not improve from -46425.18750\n",
      " Epoch 345 - NLL: -9.0196 | Reg (weighted): 0.0668 | Total approx: -8.9528\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44378.0391 - val_loss: -46316.1328\n",
      "Epoch 346/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44446.4375\n",
      "Epoch 346: val_loss did not improve from -46425.18750\n",
      " Epoch 346 - NLL: -9.0238 | Reg (weighted): 0.0670 | Total approx: -8.9568\n",
      "499/499 [==============================] - 50s 100ms/step - loss: -44446.4375 - val_loss: -46093.2812\n",
      "Epoch 347/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44440.1445\n",
      "Epoch 347: val_loss did not improve from -46425.18750\n",
      " Epoch 347 - NLL: -9.0274 | Reg (weighted): 0.0667 | Total approx: -8.9607\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44440.1445 - val_loss: -46242.1523\n",
      "Epoch 348/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44438.1641\n",
      "Epoch 348: val_loss did not improve from -46425.18750\n",
      " Epoch 348 - NLL: -9.0287 | Reg (weighted): 0.0666 | Total approx: -8.9621\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44438.1641 - val_loss: -46295.1406\n",
      "Epoch 349/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44423.7148\n",
      "Epoch 349: val_loss did not improve from -46425.18750\n",
      " Epoch 349 - NLL: -9.0098 | Reg (weighted): 0.0672 | Total approx: -8.9425\n",
      "499/499 [==============================] - 57s 114ms/step - loss: -44423.7148 - val_loss: -45776.3594\n",
      "Epoch 350/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44457.2070\n",
      "Epoch 350: val_loss did not improve from -46425.18750\n",
      " Epoch 350 - NLL: -9.0260 | Reg (weighted): 0.0667 | Total approx: -8.9593\n",
      "499/499 [==============================] - 68s 137ms/step - loss: -44457.2070 - val_loss: -46114.2148\n",
      "Epoch 351/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44480.4297\n",
      "Epoch 351: val_loss did not improve from -46425.18750\n",
      " Epoch 351 - NLL: -9.0358 | Reg (weighted): 0.0664 | Total approx: -8.9693\n",
      "499/499 [==============================] - 49s 97ms/step - loss: -44480.4297 - val_loss: -46299.0781\n",
      "Epoch 352/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44422.5859\n",
      "Epoch 352: val_loss did not improve from -46425.18750\n",
      " Epoch 352 - NLL: -9.0247 | Reg (weighted): 0.0668 | Total approx: -8.9579\n",
      "499/499 [==============================] - 53s 107ms/step - loss: -44422.5859 - val_loss: -46244.4805\n",
      "Epoch 353/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44475.4375\n",
      "Epoch 353: val_loss did not improve from -46425.18750\n",
      " Epoch 353 - NLL: -9.0326 | Reg (weighted): 0.0666 | Total approx: -8.9660\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -44475.4375 - val_loss: -46225.1992\n",
      "Epoch 354/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44435.7773\n",
      "Epoch 354: val_loss did not improve from -46425.18750\n",
      " Epoch 354 - NLL: -9.0142 | Reg (weighted): 0.0669 | Total approx: -8.9473\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -44435.7773 - val_loss: -45860.6055\n",
      "Epoch 355/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44320.6836\n",
      "Epoch 355: val_loss did not improve from -46425.18750\n",
      " Epoch 355 - NLL: -9.0088 | Reg (weighted): 0.0670 | Total approx: -8.9418\n",
      "499/499 [==============================] - 52s 105ms/step - loss: -44320.6836 - val_loss: -46274.5156\n",
      "Epoch 356/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44495.4414\n",
      "Epoch 356: val_loss did not improve from -46425.18750\n",
      " Epoch 356 - NLL: -9.0191 | Reg (weighted): 0.0666 | Total approx: -8.9525\n",
      "499/499 [==============================] - 60s 120ms/step - loss: -44495.4414 - val_loss: -45714.6328\n",
      "Epoch 357/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44479.4102\n",
      "Epoch 357: val_loss improved from -46425.18750 to -46454.42578, saving model to ./trained_models/model-0f649086-checkpoints/weights.357-t-44479.41-v-46454.43.hdf5\n",
      " Epoch 357 - NLL: -9.0408 | Reg (weighted): 0.0665 | Total approx: -8.9743\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44479.4102 - val_loss: -46454.4258\n",
      "Epoch 358/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44453.8359\n",
      "Epoch 358: val_loss did not improve from -46454.42578\n",
      " Epoch 358 - NLL: -9.0199 | Reg (weighted): 0.0666 | Total approx: -8.9533\n",
      "499/499 [==============================] - 54s 107ms/step - loss: -44453.8359 - val_loss: -45947.6406\n",
      "Epoch 359/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44504.7383\n",
      "Epoch 359: val_loss did not improve from -46454.42578\n",
      " Epoch 359 - NLL: -9.0296 | Reg (weighted): 0.0668 | Total approx: -8.9628\n",
      "499/499 [==============================] - 49s 99ms/step - loss: -44504.7383 - val_loss: -45980.2930\n",
      "Epoch 360/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44495.4805\n",
      "Epoch 360: val_loss did not improve from -46454.42578\n",
      " Epoch 360 - NLL: -9.0313 | Reg (weighted): 0.0667 | Total approx: -8.9646\n",
      "499/499 [==============================] - 59s 119ms/step - loss: -44495.4805 - val_loss: -46080.6836\n",
      "Epoch 361/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44499.2148\n",
      "Epoch 361: val_loss did not improve from -46454.42578\n",
      " Epoch 361 - NLL: -9.0271 | Reg (weighted): 0.0667 | Total approx: -8.9605\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -44499.2148 - val_loss: -45936.4766\n",
      "Epoch 362/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44490.8945\n",
      "Epoch 362: val_loss did not improve from -46454.42578\n",
      " Epoch 362 - NLL: -9.0378 | Reg (weighted): 0.0668 | Total approx: -8.9710\n",
      "499/499 [==============================] - 57s 114ms/step - loss: -44490.8945 - val_loss: -46297.4805\n",
      "Epoch 363/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44490.3164\n",
      "Epoch 363: val_loss did not improve from -46454.42578\n",
      " Epoch 363 - NLL: -9.0348 | Reg (weighted): 0.0668 | Total approx: -8.9680\n",
      "499/499 [==============================] - 54s 107ms/step - loss: -44490.3164 - val_loss: -46209.3906\n",
      "Epoch 364/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44498.4688\n",
      "Epoch 364: val_loss did not improve from -46454.42578\n",
      " Epoch 364 - NLL: -9.0381 | Reg (weighted): 0.0666 | Total approx: -8.9715\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -44498.4688 - val_loss: -46272.3359\n",
      "Epoch 365/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44453.8867\n",
      "Epoch 365: val_loss did not improve from -46454.42578\n",
      " Epoch 365 - NLL: -9.0095 | Reg (weighted): 0.0669 | Total approx: -8.9426\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44453.8867 - val_loss: -45625.8945\n",
      "Epoch 366/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44511.8906\n",
      "Epoch 366: val_loss did not improve from -46454.42578\n",
      " Epoch 366 - NLL: -9.0440 | Reg (weighted): 0.0667 | Total approx: -8.9773\n",
      "499/499 [==============================] - 56s 112ms/step - loss: -44511.8906 - val_loss: -46379.4102\n",
      "Epoch 367/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44510.7148\n",
      "Epoch 367: val_loss did not improve from -46454.42578\n",
      " Epoch 367 - NLL: -9.0394 | Reg (weighted): 0.0663 | Total approx: -8.9731\n",
      "499/499 [==============================] - 48s 97ms/step - loss: -44510.7148 - val_loss: -46259.2852\n",
      "Epoch 368/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44519.2578\n",
      "Epoch 368: val_loss did not improve from -46454.42578\n",
      " Epoch 368 - NLL: -9.0317 | Reg (weighted): 0.0664 | Total approx: -8.9653\n",
      "499/499 [==============================] - 61s 123ms/step - loss: -44519.2578 - val_loss: -45983.1133\n",
      "Epoch 369/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44544.0781\n",
      "Epoch 369: val_loss did not improve from -46454.42578\n",
      " Epoch 369 - NLL: -9.0437 | Reg (weighted): 0.0665 | Total approx: -8.9771\n",
      "499/499 [==============================] - 48s 97ms/step - loss: -44544.0781 - val_loss: -46213.5352\n",
      "Epoch 370/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44505.5117\n",
      "Epoch 370: val_loss did not improve from -46454.42578\n",
      " Epoch 370 - NLL: -9.0333 | Reg (weighted): 0.0666 | Total approx: -8.9667\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44505.5117 - val_loss: -46091.7070\n",
      "Epoch 371/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44484.5508\n",
      "Epoch 371: val_loss did not improve from -46454.42578\n",
      " Epoch 371 - NLL: -9.0170 | Reg (weighted): 0.0667 | Total approx: -8.9503\n",
      "499/499 [==============================] - 49s 99ms/step - loss: -44484.5508 - val_loss: -45704.7031\n",
      "Epoch 372/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44547.0820\n",
      "Epoch 372: val_loss did not improve from -46454.42578\n",
      " Epoch 372 - NLL: -9.0487 | Reg (weighted): 0.0664 | Total approx: -8.9823\n",
      "499/499 [==============================] - 58s 116ms/step - loss: -44547.0820 - val_loss: -46353.9844\n",
      "Epoch 373/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44526.6641\n",
      "Epoch 373: val_loss did not improve from -46454.42578\n",
      " Epoch 373 - NLL: -9.0431 | Reg (weighted): 0.0664 | Total approx: -8.9767\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -44526.6641 - val_loss: -46288.1758\n",
      "Epoch 374/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44539.6992\n",
      "Epoch 374: val_loss did not improve from -46454.42578\n",
      " Epoch 374 - NLL: -9.0468 | Reg (weighted): 0.0663 | Total approx: -8.9805\n",
      "499/499 [==============================] - 67s 134ms/step - loss: -44539.6992 - val_loss: -46336.5312\n",
      "Epoch 375/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44552.4609\n",
      "Epoch 375: val_loss did not improve from -46454.42578\n",
      " Epoch 375 - NLL: -9.0479 | Reg (weighted): 0.0663 | Total approx: -8.9816\n",
      "499/499 [==============================] - 52s 105ms/step - loss: -44552.4609 - val_loss: -46305.7969\n",
      "Epoch 376/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44543.3594\n",
      "Epoch 376: val_loss did not improve from -46454.42578\n",
      " Epoch 376 - NLL: -9.0366 | Reg (weighted): 0.0662 | Total approx: -8.9704\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -44543.3594 - val_loss: -46015.2578\n",
      "Epoch 377/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44495.8828\n",
      "Epoch 377: val_loss did not improve from -46454.42578\n",
      " Epoch 377 - NLL: -9.0362 | Reg (weighted): 0.0662 | Total approx: -8.9700\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -44495.8828 - val_loss: -46239.7461\n",
      "Epoch 378/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44557.2227\n",
      "Epoch 378: val_loss did not improve from -46454.42578\n",
      " Epoch 378 - NLL: -9.0315 | Reg (weighted): 0.0666 | Total approx: -8.9649\n",
      "499/499 [==============================] - 53s 107ms/step - loss: -44557.2227 - val_loss: -45779.4766\n",
      "Epoch 379/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44533.1133\n",
      "Epoch 379: val_loss did not improve from -46454.42578\n",
      " Epoch 379 - NLL: -9.0339 | Reg (weighted): 0.0665 | Total approx: -8.9675\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44533.1133 - val_loss: -45977.2656\n",
      "Epoch 380/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44592.1758\n",
      "Epoch 380: val_loss did not improve from -46454.42578\n",
      " Epoch 380 - NLL: -9.0537 | Reg (weighted): 0.0661 | Total approx: -8.9876\n",
      "499/499 [==============================] - 57s 114ms/step - loss: -44592.1758 - val_loss: -46286.0664\n",
      "Epoch 381/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44577.8047\n",
      "Epoch 381: val_loss did not improve from -46454.42578\n",
      " Epoch 381 - NLL: -9.0461 | Reg (weighted): 0.0666 | Total approx: -8.9795\n",
      "499/499 [==============================] - 50s 99ms/step - loss: -44577.8047 - val_loss: -46116.0352\n",
      "Epoch 382/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44588.0898\n",
      "Epoch 382: val_loss did not improve from -46454.42578\n",
      " Epoch 382 - NLL: -9.0525 | Reg (weighted): 0.0663 | Total approx: -8.9862\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44588.0898 - val_loss: -46265.4102\n",
      "Epoch 383/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44548.5156\n",
      "Epoch 383: val_loss did not improve from -46454.42578\n",
      " Epoch 383 - NLL: -9.0428 | Reg (weighted): 0.0666 | Total approx: -8.9763\n",
      "499/499 [==============================] - 51s 103ms/step - loss: -44548.5156 - val_loss: -46164.6797\n",
      "Epoch 384/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44560.5273\n",
      "Epoch 384: val_loss did not improve from -46454.42578\n",
      " Epoch 384 - NLL: -9.0448 | Reg (weighted): 0.0665 | Total approx: -8.9783\n",
      "499/499 [==============================] - 53s 107ms/step - loss: -44560.5273 - val_loss: -46165.3516\n",
      "Epoch 385/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44573.7031\n",
      "Epoch 385: val_loss did not improve from -46454.42578\n",
      " Epoch 385 - NLL: -9.0396 | Reg (weighted): 0.0664 | Total approx: -8.9732\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44573.7031 - val_loss: -45946.3594\n",
      "Epoch 386/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44585.2578\n",
      "Epoch 386: val_loss did not improve from -46454.42578\n",
      " Epoch 386 - NLL: -9.0259 | Reg (weighted): 0.0667 | Total approx: -8.9592\n",
      "499/499 [==============================] - 57s 114ms/step - loss: -44585.2578 - val_loss: -45464.8867\n",
      "Epoch 387/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44613.0000\n",
      "Epoch 387: val_loss did not improve from -46454.42578\n",
      " Epoch 387 - NLL: -9.0544 | Reg (weighted): 0.0663 | Total approx: -8.9881\n",
      "499/499 [==============================] - 46s 93ms/step - loss: -44613.0000 - val_loss: -46198.5820\n",
      "Epoch 388/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44490.3359\n",
      "Epoch 388: val_loss did not improve from -46454.42578\n",
      " Epoch 388 - NLL: -9.0406 | Reg (weighted): 0.0664 | Total approx: -8.9742\n",
      "499/499 [==============================] - 45s 90ms/step - loss: -44490.3359 - val_loss: -46394.3438\n",
      "Epoch 389/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44591.7930\n",
      "Epoch 389: val_loss did not improve from -46454.42578\n",
      " Epoch 389 - NLL: -9.0541 | Reg (weighted): 0.0665 | Total approx: -8.9876\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44591.7930 - val_loss: -46289.0195\n",
      "Epoch 390/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44624.9023\n",
      "Epoch 390: val_loss did not improve from -46454.42578\n",
      " Epoch 390 - NLL: -9.0617 | Reg (weighted): 0.0660 | Total approx: -8.9956\n",
      "499/499 [==============================] - 51s 103ms/step - loss: -44624.9023 - val_loss: -46363.1641\n",
      "Epoch 391/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44585.9727\n",
      "Epoch 391: val_loss did not improve from -46454.42578\n",
      " Epoch 391 - NLL: -9.0419 | Reg (weighted): 0.0662 | Total approx: -8.9757\n",
      "499/499 [==============================] - 46s 92ms/step - loss: -44585.9727 - val_loss: -45958.8789\n",
      "Epoch 392/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44579.4180\n",
      "Epoch 392: val_loss did not improve from -46454.42578\n",
      " Epoch 392 - NLL: -9.0307 | Reg (weighted): 0.0661 | Total approx: -8.9645\n",
      "499/499 [==============================] - 47s 95ms/step - loss: -44579.4180 - val_loss: -45655.1836\n",
      "Epoch 393/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44606.7539\n",
      "Epoch 393: val_loss did not improve from -46454.42578\n",
      " Epoch 393 - NLL: -9.0602 | Reg (weighted): 0.0665 | Total approx: -8.9937\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -44606.7539 - val_loss: -46396.9961\n",
      "Epoch 394/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44582.9141\n",
      "Epoch 394: val_loss did not improve from -46454.42578\n",
      " Epoch 394 - NLL: -9.0514 | Reg (weighted): 0.0665 | Total approx: -8.9849\n",
      "499/499 [==============================] - 45s 90ms/step - loss: -44582.9141 - val_loss: -46251.9453\n",
      "Epoch 395/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44635.6875\n",
      "Epoch 395: val_loss did not improve from -46454.42578\n",
      " Epoch 395 - NLL: -9.0567 | Reg (weighted): 0.0659 | Total approx: -8.9908\n",
      "499/499 [==============================] - 47s 93ms/step - loss: -44635.6875 - val_loss: -46164.1328\n",
      "Epoch 396/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44603.4648\n",
      "Epoch 396: val_loss did not improve from -46454.42578\n",
      " Epoch 396 - NLL: -9.0546 | Reg (weighted): 0.0660 | Total approx: -8.9885\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -44603.4648 - val_loss: -46257.6758\n",
      "Epoch 397/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44608.7773\n",
      "Epoch 397: val_loss did not improve from -46454.42578\n",
      " Epoch 397 - NLL: -9.0581 | Reg (weighted): 0.0663 | Total approx: -8.9918\n",
      "499/499 [==============================] - 47s 94ms/step - loss: -44608.7773 - val_loss: -46328.8555\n",
      "Epoch 398/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44623.5391\n",
      "Epoch 398: val_loss did not improve from -46454.42578\n",
      " Epoch 398 - NLL: -9.0579 | Reg (weighted): 0.0661 | Total approx: -8.9918\n",
      "499/499 [==============================] - 49s 99ms/step - loss: -44623.5391 - val_loss: -46254.4141\n",
      "Epoch 399/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44625.7344\n",
      "Epoch 399: val_loss improved from -46454.42578 to -46533.13281, saving model to ./trained_models/model-0f649086-checkpoints/weights.399-t-44625.73-v-46533.13.hdf5\n",
      " Epoch 399 - NLL: -9.0674 | Reg (weighted): 0.0661 | Total approx: -9.0014\n",
      "499/499 [==============================] - 52s 105ms/step - loss: -44625.7344 - val_loss: -46533.1328\n",
      "Epoch 400/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44634.4648\n",
      "Epoch 400: val_loss did not improve from -46533.13281\n",
      " Epoch 400 - NLL: -9.0554 | Reg (weighted): 0.0661 | Total approx: -8.9893\n",
      "499/499 [==============================] - 47s 93ms/step - loss: -44634.4648 - val_loss: -46125.2500\n",
      "Epoch 401/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44619.1445\n",
      "Epoch 401: val_loss did not improve from -46533.13281\n",
      " Epoch 401 - NLL: -9.0489 | Reg (weighted): 0.0662 | Total approx: -8.9827\n",
      "499/499 [==============================] - 44s 89ms/step - loss: -44619.1445 - val_loss: -46004.1562\n",
      "Epoch 402/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44587.9062\n",
      "Epoch 402: val_loss did not improve from -46533.13281\n",
      " Epoch 402 - NLL: -9.0529 | Reg (weighted): 0.0663 | Total approx: -8.9866\n",
      "499/499 [==============================] - 50s 101ms/step - loss: -44587.9062 - val_loss: -46278.0703\n",
      "Epoch 403/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44646.4922\n",
      "Epoch 403: val_loss improved from -46533.13281 to -46652.05859, saving model to ./trained_models/model-0f649086-checkpoints/weights.403-t-44646.49-v-46652.06.hdf5\n",
      " Epoch 403 - NLL: -9.0749 | Reg (weighted): 0.0661 | Total approx: -9.0088\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -44646.4922 - val_loss: -46652.0586\n",
      "Epoch 404/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44612.3633\n",
      "Epoch 404: val_loss did not improve from -46652.05859\n",
      " Epoch 404 - NLL: -9.0539 | Reg (weighted): 0.0664 | Total approx: -8.9874\n",
      "499/499 [==============================] - 46s 91ms/step - loss: -44612.3633 - val_loss: -46180.0898\n",
      "Epoch 405/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44655.8555\n",
      "Epoch 405: val_loss did not improve from -46652.05859\n",
      " Epoch 405 - NLL: -9.0703 | Reg (weighted): 0.0659 | Total approx: -9.0044\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -44655.8555 - val_loss: -46472.3711\n",
      "Epoch 406/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44627.3320\n",
      "Epoch 406: val_loss did not improve from -46652.05859\n",
      " Epoch 406 - NLL: -9.0483 | Reg (weighted): 0.0661 | Total approx: -8.9821\n",
      "499/499 [==============================] - 52s 105ms/step - loss: -44627.3320 - val_loss: -45944.2305\n",
      "Epoch 407/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44673.3164\n",
      "Epoch 407: val_loss did not improve from -46652.05859\n",
      " Epoch 407 - NLL: -9.0683 | Reg (weighted): 0.0661 | Total approx: -9.0022\n",
      "499/499 [==============================] - 45s 91ms/step - loss: -44673.3164 - val_loss: -46316.8477\n",
      "Epoch 408/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44547.7031\n",
      "Epoch 408: val_loss did not improve from -46652.05859\n",
      " Epoch 408 - NLL: -8.9914 | Reg (weighted): 0.0668 | Total approx: -8.9246\n",
      "499/499 [==============================] - 50s 100ms/step - loss: -44547.7031 - val_loss: -44610.4375\n",
      "Epoch 409/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44646.6445\n",
      "Epoch 409: val_loss did not improve from -46652.05859\n",
      " Epoch 409 - NLL: -9.0641 | Reg (weighted): 0.0662 | Total approx: -8.9979\n",
      "499/499 [==============================] - 50s 101ms/step - loss: -44646.6445 - val_loss: -46321.3281\n",
      "Epoch 410/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44650.4805\n",
      "Epoch 410: val_loss did not improve from -46652.05859\n",
      " Epoch 410 - NLL: -9.0592 | Reg (weighted): 0.0661 | Total approx: -8.9931\n",
      "499/499 [==============================] - 47s 95ms/step - loss: -44650.4805 - val_loss: -46157.8906\n",
      "Epoch 411/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44669.6523\n",
      "Epoch 411: val_loss did not improve from -46652.05859\n",
      " Epoch 411 - NLL: -9.0663 | Reg (weighted): 0.0663 | Total approx: -9.0001\n",
      "499/499 [==============================] - 45s 90ms/step - loss: -44669.6523 - val_loss: -46272.1758\n",
      "Epoch 412/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44673.6680\n",
      "Epoch 412: val_loss did not improve from -46652.05859\n",
      " Epoch 412 - NLL: -9.0607 | Reg (weighted): 0.0662 | Total approx: -8.9945\n",
      "499/499 [==============================] - 53s 105ms/step - loss: -44673.6680 - val_loss: -46083.6875\n",
      "Epoch 413/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44638.3086\n",
      "Epoch 413: val_loss did not improve from -46652.05859\n",
      " Epoch 413 - NLL: -9.0642 | Reg (weighted): 0.0660 | Total approx: -8.9982\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44638.3086 - val_loss: -46374.3438\n",
      "Epoch 414/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44670.4023\n",
      "Epoch 414: val_loss did not improve from -46652.05859\n",
      " Epoch 414 - NLL: -9.0671 | Reg (weighted): 0.0659 | Total approx: -9.0012\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -44670.4023 - val_loss: -46301.3008\n",
      "Epoch 415/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44667.5195\n",
      "Epoch 415: val_loss did not improve from -46652.05859\n",
      " Epoch 415 - NLL: -9.0608 | Reg (weighted): 0.0664 | Total approx: -8.9944\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -44667.5195 - val_loss: -46110.7266\n",
      "Epoch 416/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44696.4180\n",
      "Epoch 416: val_loss did not improve from -46652.05859\n",
      " Epoch 416 - NLL: -9.0714 | Reg (weighted): 0.0661 | Total approx: -9.0053\n",
      "499/499 [==============================] - 53s 105ms/step - loss: -44696.4180 - val_loss: -46294.3242\n",
      "Epoch 417/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44618.4180\n",
      "Epoch 417: val_loss did not improve from -46652.05859\n",
      " Epoch 417 - NLL: -9.0547 | Reg (weighted): 0.0662 | Total approx: -8.9885\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -44618.4180 - val_loss: -46181.5625\n",
      "Epoch 418/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44669.6562\n",
      "Epoch 418: val_loss did not improve from -46652.05859\n",
      " Epoch 418 - NLL: -9.0569 | Reg (weighted): 0.0661 | Total approx: -8.9908\n",
      "499/499 [==============================] - 52s 105ms/step - loss: -44669.6562 - val_loss: -45993.2422\n",
      "Epoch 419/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44688.7422\n",
      "Epoch 419: val_loss did not improve from -46652.05859\n",
      " Epoch 419 - NLL: -9.0680 | Reg (weighted): 0.0660 | Total approx: -9.0020\n",
      "499/499 [==============================] - 56s 113ms/step - loss: -44688.7422 - val_loss: -46232.9531\n",
      "Epoch 420/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44707.8789\n",
      "Epoch 420: val_loss did not improve from -46652.05859\n",
      " Epoch 420 - NLL: -9.0702 | Reg (weighted): 0.0661 | Total approx: -9.0041\n",
      "499/499 [==============================] - 52s 105ms/step - loss: -44707.8789 - val_loss: -46200.1367\n",
      "Epoch 421/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44660.4180\n",
      "Epoch 421: val_loss did not improve from -46652.05859\n",
      " Epoch 421 - NLL: -9.0641 | Reg (weighted): 0.0660 | Total approx: -8.9982\n",
      "499/499 [==============================] - 50s 101ms/step - loss: -44660.4180 - val_loss: -46260.9062\n",
      "Epoch 422/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44684.9336\n",
      "Epoch 422: val_loss did not improve from -46652.05859\n",
      " Epoch 422 - NLL: -9.0717 | Reg (weighted): 0.0660 | Total approx: -9.0057\n",
      "499/499 [==============================] - 53s 105ms/step - loss: -44684.9336 - val_loss: -46365.7656\n",
      "Epoch 423/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44701.3086\n",
      "Epoch 423: val_loss did not improve from -46652.05859\n",
      " Epoch 423 - NLL: -9.0662 | Reg (weighted): 0.0660 | Total approx: -9.0003\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -44701.3086 - val_loss: -46118.3555\n",
      "Epoch 424/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44704.7812\n",
      "Epoch 424: val_loss did not improve from -46652.05859\n",
      " Epoch 424 - NLL: -9.0714 | Reg (weighted): 0.0659 | Total approx: -9.0055\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44704.7812 - val_loss: -46259.4805\n",
      "Epoch 425/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44683.8125\n",
      "Epoch 425: val_loss did not improve from -46652.05859\n",
      " Epoch 425 - NLL: -9.0736 | Reg (weighted): 0.0658 | Total approx: -9.0078\n",
      "499/499 [==============================] - 63s 126ms/step - loss: -44683.8125 - val_loss: -46433.8984\n",
      "Epoch 426/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44706.7031\n",
      "Epoch 426: val_loss did not improve from -46652.05859\n",
      " Epoch 426 - NLL: -9.0699 | Reg (weighted): 0.0661 | Total approx: -9.0038\n",
      "499/499 [==============================] - 56s 112ms/step - loss: -44706.7031 - val_loss: -46198.9805\n",
      "Epoch 427/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44736.4141\n",
      "Epoch 427: val_loss did not improve from -46652.05859\n",
      " Epoch 427 - NLL: -9.0887 | Reg (weighted): 0.0660 | Total approx: -9.0227\n",
      "499/499 [==============================] - 54s 107ms/step - loss: -44736.4141 - val_loss: -46618.0391\n",
      "Epoch 428/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44692.6328\n",
      "Epoch 428: val_loss did not improve from -46652.05859\n",
      " Epoch 428 - NLL: -9.0760 | Reg (weighted): 0.0658 | Total approx: -9.0102\n",
      "499/499 [==============================] - 50s 99ms/step - loss: -44692.6328 - val_loss: -46463.0859\n",
      "Epoch 429/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44768.0977\n",
      "Epoch 429: val_loss did not improve from -46652.05859\n",
      " Epoch 429 - NLL: -9.0897 | Reg (weighted): 0.0662 | Total approx: -9.0235\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -44768.0977 - val_loss: -46482.1211\n",
      "Epoch 430/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44754.2344\n",
      "Epoch 430: val_loss improved from -46652.05859 to -46753.64453, saving model to ./trained_models/model-0f649086-checkpoints/weights.430-t-44754.23-v-46753.64.hdf5\n",
      " Epoch 430 - NLL: -9.0960 | Reg (weighted): 0.0658 | Total approx: -9.0302\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -44754.2344 - val_loss: -46753.6445\n",
      "Epoch 431/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44757.2031\n",
      "Epoch 431: val_loss did not improve from -46753.64453\n",
      " Epoch 431 - NLL: -9.0938 | Reg (weighted): 0.0660 | Total approx: -9.0277\n",
      "499/499 [==============================] - 56s 112ms/step - loss: -44757.2031 - val_loss: -46665.0859\n",
      "Epoch 432/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44760.0586\n",
      "Epoch 432: val_loss did not improve from -46753.64453\n",
      " Epoch 432 - NLL: -9.0797 | Reg (weighted): 0.0659 | Total approx: -9.0138\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -44760.0586 - val_loss: -46229.5352\n",
      "Epoch 433/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44780.6289\n",
      "Epoch 433: val_loss did not improve from -46753.64453\n",
      " Epoch 433 - NLL: -9.0879 | Reg (weighted): 0.0662 | Total approx: -9.0217\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -44780.6289 - val_loss: -46365.8906\n",
      "Epoch 434/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44789.8516\n",
      "Epoch 434: val_loss did not improve from -46753.64453\n",
      " Epoch 434 - NLL: -9.0781 | Reg (weighted): 0.0663 | Total approx: -9.0118\n",
      "499/499 [==============================] - 53s 105ms/step - loss: -44789.8516 - val_loss: -46019.7969\n",
      "Epoch 435/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44786.6758\n",
      "Epoch 435: val_loss did not improve from -46753.64453\n",
      " Epoch 435 - NLL: -9.0985 | Reg (weighted): 0.0662 | Total approx: -9.0323\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44786.6758 - val_loss: -46654.2227\n",
      "Epoch 436/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44758.5430\n",
      "Epoch 436: val_loss did not improve from -46753.64453\n",
      " Epoch 436 - NLL: -9.0564 | Reg (weighted): 0.0662 | Total approx: -8.9902\n",
      "499/499 [==============================] - 48s 97ms/step - loss: -44758.5430 - val_loss: -45527.0703\n",
      "Epoch 437/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44793.0938\n",
      "Epoch 437: val_loss did not improve from -46753.64453\n",
      " Epoch 437 - NLL: -9.0951 | Reg (weighted): 0.0663 | Total approx: -9.0288\n",
      "499/499 [==============================] - 55s 110ms/step - loss: -44793.0938 - val_loss: -46517.2148\n",
      "Epoch 438/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44785.4844\n",
      "Epoch 438: val_loss did not improve from -46753.64453\n",
      " Epoch 438 - NLL: -9.0937 | Reg (weighted): 0.0660 | Total approx: -9.0277\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -44785.4844 - val_loss: -46521.3164\n",
      "Epoch 439/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44833.2227\n",
      "Epoch 439: val_loss did not improve from -46753.64453\n",
      " Epoch 439 - NLL: -9.1036 | Reg (weighted): 0.0663 | Total approx: -9.0373\n",
      "499/499 [==============================] - 54s 108ms/step - loss: -44833.2227 - val_loss: -46570.8867\n",
      "Epoch 440/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44801.1680\n",
      "Epoch 440: val_loss did not improve from -46753.64453\n",
      " Epoch 440 - NLL: -9.0946 | Reg (weighted): 0.0659 | Total approx: -9.0287\n",
      "499/499 [==============================] - 51s 101ms/step - loss: -44801.1680 - val_loss: -46471.9922\n",
      "Epoch 441/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44801.1562\n",
      "Epoch 441: val_loss did not improve from -46753.64453\n",
      " Epoch 441 - NLL: -9.0922 | Reg (weighted): 0.0661 | Total approx: -9.0260\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -44801.1562 - val_loss: -46392.4141\n",
      "Epoch 442/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44833.5273\n",
      "Epoch 442: val_loss did not improve from -46753.64453\n",
      " Epoch 442 - NLL: -9.0942 | Reg (weighted): 0.0657 | Total approx: -9.0285\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44833.5273 - val_loss: -46304.1602\n",
      "Epoch 443/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44817.9023\n",
      "Epoch 443: val_loss did not improve from -46753.64453\n",
      " Epoch 443 - NLL: -9.0952 | Reg (weighted): 0.0659 | Total approx: -9.0292\n",
      "499/499 [==============================] - 55s 111ms/step - loss: -44817.9023 - val_loss: -46404.5391\n",
      "Epoch 444/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44794.4180\n",
      "Epoch 444: val_loss did not improve from -46753.64453\n",
      " Epoch 444 - NLL: -9.0842 | Reg (weighted): 0.0659 | Total approx: -9.0183\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -44794.4180 - val_loss: -46192.8789\n",
      "Epoch 445/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44832.4102\n",
      "Epoch 445: val_loss did not improve from -46753.64453\n",
      " Epoch 445 - NLL: -9.0951 | Reg (weighted): 0.0657 | Total approx: -9.0294\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -44832.4102 - val_loss: -46336.5000\n",
      "Epoch 446/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44828.9531\n",
      "Epoch 446: val_loss did not improve from -46753.64453\n",
      " Epoch 446 - NLL: -9.0977 | Reg (weighted): 0.0660 | Total approx: -9.0317\n",
      "499/499 [==============================] - 53s 107ms/step - loss: -44828.9531 - val_loss: -46422.6562\n",
      "Epoch 447/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44826.7227\n",
      "Epoch 447: val_loss improved from -46753.64453 to -46960.91016, saving model to ./trained_models/model-0f649086-checkpoints/weights.447-t-44826.72-v-46960.91.hdf5\n",
      " Epoch 447 - NLL: -9.1147 | Reg (weighted): 0.0655 | Total approx: -9.0492\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44826.7227 - val_loss: -46960.9102\n",
      "Epoch 448/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44854.5586\n",
      "Epoch 448: val_loss did not improve from -46960.91016\n",
      " Epoch 448 - NLL: -9.0976 | Reg (weighted): 0.0659 | Total approx: -9.0316\n",
      "499/499 [==============================] - 48s 97ms/step - loss: -44854.5586 - val_loss: -46291.9727\n",
      "Epoch 449/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44837.8203\n",
      "Epoch 449: val_loss did not improve from -46960.91016\n",
      " Epoch 449 - NLL: -9.0977 | Reg (weighted): 0.0657 | Total approx: -9.0321\n",
      "499/499 [==============================] - 55s 110ms/step - loss: -44837.8203 - val_loss: -46389.0625\n",
      "Epoch 450/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44810.3047\n",
      "Epoch 450: val_loss did not improve from -46960.91016\n",
      " Epoch 450 - NLL: -9.1010 | Reg (weighted): 0.0659 | Total approx: -9.0351\n",
      "499/499 [==============================] - 53s 107ms/step - loss: -44810.3047 - val_loss: -46618.4180\n",
      "Epoch 451/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44818.2734\n",
      "Epoch 451: val_loss did not improve from -46960.91016\n",
      " Epoch 451 - NLL: -9.0992 | Reg (weighted): 0.0659 | Total approx: -9.0334\n",
      "499/499 [==============================] - 51s 103ms/step - loss: -44818.2734 - val_loss: -46527.9805\n",
      "Epoch 452/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44861.3047\n",
      "Epoch 452: val_loss did not improve from -46960.91016\n",
      " Epoch 452 - NLL: -9.1036 | Reg (weighted): 0.0659 | Total approx: -9.0377\n",
      "499/499 [==============================] - 50s 100ms/step - loss: -44861.3047 - val_loss: -46441.4570\n",
      "Epoch 453/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44857.9648\n",
      "Epoch 453: val_loss did not improve from -46960.91016\n",
      " Epoch 453 - NLL: -9.1030 | Reg (weighted): 0.0660 | Total approx: -9.0370\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -44857.9648 - val_loss: -46436.9805\n",
      "Epoch 454/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44854.6562\n",
      "Epoch 454: val_loss did not improve from -46960.91016\n",
      " Epoch 454 - NLL: -9.0906 | Reg (weighted): 0.0659 | Total approx: -9.0247\n",
      "499/499 [==============================] - 48s 95ms/step - loss: -44854.6562 - val_loss: -46082.0352\n",
      "Epoch 455/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44847.0156\n",
      "Epoch 455: val_loss did not improve from -46960.91016\n",
      " Epoch 455 - NLL: -9.1030 | Reg (weighted): 0.0659 | Total approx: -9.0370\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -44847.0156 - val_loss: -46493.2383\n",
      "Epoch 456/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44834.1367\n",
      "Epoch 456: val_loss did not improve from -46960.91016\n",
      " Epoch 456 - NLL: -9.1117 | Reg (weighted): 0.0659 | Total approx: -9.0459\n",
      "499/499 [==============================] - 55s 111ms/step - loss: -44834.1367 - val_loss: -46823.8555\n",
      "Epoch 457/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44874.1914\n",
      "Epoch 457: val_loss did not improve from -46960.91016\n",
      " Epoch 457 - NLL: -9.0934 | Reg (weighted): 0.0660 | Total approx: -9.0274\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -44874.1914 - val_loss: -46064.4023\n",
      "Epoch 458/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44852.5234\n",
      "Epoch 458: val_loss did not improve from -46960.91016\n",
      " Epoch 458 - NLL: -9.0923 | Reg (weighted): 0.0662 | Total approx: -9.0260\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -44852.5234 - val_loss: -46133.8164\n",
      "Epoch 459/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44866.8164\n",
      "Epoch 459: val_loss did not improve from -46960.91016\n",
      " Epoch 459 - NLL: -9.1118 | Reg (weighted): 0.0656 | Total approx: -9.0462\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -44866.8164 - val_loss: -46669.3398\n",
      "Epoch 460/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44888.9531\n",
      "Epoch 460: val_loss did not improve from -46960.91016\n",
      " Epoch 460 - NLL: -9.0973 | Reg (weighted): 0.0659 | Total approx: -9.0314\n",
      "499/499 [==============================] - 56s 112ms/step - loss: -44888.9531 - val_loss: -46111.6641\n",
      "Epoch 461/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44878.8359\n",
      "Epoch 461: val_loss did not improve from -46960.91016\n",
      " Epoch 461 - NLL: -9.1171 | Reg (weighted): 0.0659 | Total approx: -9.0512\n",
      "499/499 [==============================] - 53s 107ms/step - loss: -44878.8359 - val_loss: -46758.4375\n",
      "Epoch 462/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44893.4336\n",
      "Epoch 462: val_loss did not improve from -46960.91016\n",
      " Epoch 462 - NLL: -9.1105 | Reg (weighted): 0.0657 | Total approx: -9.0447\n",
      "499/499 [==============================] - 53s 107ms/step - loss: -44893.4336 - val_loss: -46491.3906\n",
      "Epoch 463/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44862.1562\n",
      "Epoch 463: val_loss did not improve from -46960.91016\n",
      " Epoch 463 - NLL: -9.0928 | Reg (weighted): 0.0656 | Total approx: -9.0273\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -44862.1562 - val_loss: -46122.6641\n",
      "Epoch 464/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44886.7266\n",
      "Epoch 464: val_loss did not improve from -46960.91016\n",
      " Epoch 464 - NLL: -9.1139 | Reg (weighted): 0.0656 | Total approx: -9.0482\n",
      "499/499 [==============================] - 52s 103ms/step - loss: -44886.7266 - val_loss: -46630.1758\n",
      "Epoch 465/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44862.4492\n",
      "Epoch 465: val_loss did not improve from -46960.91016\n",
      " Epoch 465 - NLL: -9.0954 | Reg (weighted): 0.0661 | Total approx: -9.0293\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44862.4492 - val_loss: -46180.8008\n",
      "Epoch 466/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44795.1953\n",
      "Epoch 466: val_loss did not improve from -46960.91016\n",
      " Epoch 466 - NLL: -9.0912 | Reg (weighted): 0.0658 | Total approx: -9.0254\n",
      "499/499 [==============================] - 49s 99ms/step - loss: -44795.1953 - val_loss: -46404.1602\n",
      "Epoch 467/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44895.1328\n",
      "Epoch 467: val_loss did not improve from -46960.91016\n",
      " Epoch 467 - NLL: -9.1062 | Reg (weighted): 0.0656 | Total approx: -9.0406\n",
      "499/499 [==============================] - 52s 105ms/step - loss: -44895.1328 - val_loss: -46357.2773\n",
      "Epoch 468/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44888.7578\n",
      "Epoch 468: val_loss did not improve from -46960.91016\n",
      " Epoch 468 - NLL: -9.0945 | Reg (weighted): 0.0660 | Total approx: -9.0284\n",
      "499/499 [==============================] - 59s 117ms/step - loss: -44888.7578 - val_loss: -46022.5000\n",
      "Epoch 469/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44887.3867\n",
      "Epoch 469: val_loss did not improve from -46960.91016\n",
      " Epoch 469 - NLL: -9.1055 | Reg (weighted): 0.0658 | Total approx: -9.0398\n",
      "499/499 [==============================] - 55s 111ms/step - loss: -44887.3867 - val_loss: -46371.3281\n",
      "Epoch 470/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44773.7695\n",
      "Epoch 470: val_loss did not improve from -46960.91016\n",
      " Epoch 470 - NLL: -9.0983 | Reg (weighted): 0.0658 | Total approx: -9.0325\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -44773.7695 - val_loss: -46724.8242\n",
      "Epoch 471/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44918.3672\n",
      "Epoch 471: val_loss did not improve from -46960.91016\n",
      " Epoch 471 - NLL: -9.1165 | Reg (weighted): 0.0658 | Total approx: -9.0507\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44918.3672 - val_loss: -46544.8398\n",
      "Epoch 472/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44914.8867\n",
      "Epoch 472: val_loss did not improve from -46960.91016\n",
      " Epoch 472 - NLL: -9.1121 | Reg (weighted): 0.0654 | Total approx: -9.0467\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44914.8867 - val_loss: -46442.7656\n",
      "Epoch 473/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44881.5000\n",
      "Epoch 473: val_loss did not improve from -46960.91016\n",
      " Epoch 473 - NLL: -9.1177 | Reg (weighted): 0.0657 | Total approx: -9.0520\n",
      "499/499 [==============================] - 52s 105ms/step - loss: -44881.5000 - val_loss: -46770.8125\n",
      "Epoch 474/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44908.2773\n",
      "Epoch 474: val_loss did not improve from -46960.91016\n",
      " Epoch 474 - NLL: -9.1070 | Reg (weighted): 0.0660 | Total approx: -9.0411\n",
      "499/499 [==============================] - 58s 116ms/step - loss: -44908.2773 - val_loss: -46305.4688\n",
      "Epoch 475/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44913.9648\n",
      "Epoch 475: val_loss did not improve from -46960.91016\n",
      " Epoch 475 - NLL: -9.1079 | Reg (weighted): 0.0657 | Total approx: -9.0422\n",
      "499/499 [==============================] - 54s 107ms/step - loss: -44913.9648 - val_loss: -46310.1328\n",
      "Epoch 476/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44891.5586\n",
      "Epoch 476: val_loss did not improve from -46960.91016\n",
      " Epoch 476 - NLL: -9.1163 | Reg (weighted): 0.0657 | Total approx: -9.0506\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -44891.5586 - val_loss: -46677.1797\n",
      "Epoch 477/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44939.5312\n",
      "Epoch 477: val_loss did not improve from -46960.91016\n",
      " Epoch 477 - NLL: -9.1288 | Reg (weighted): 0.0656 | Total approx: -9.0632\n",
      "499/499 [==============================] - 47s 94ms/step - loss: -44939.5312 - val_loss: -46816.6289\n",
      "Epoch 478/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44720.0625\n",
      "Epoch 478: val_loss did not improve from -46960.91016\n",
      " Epoch 478 - NLL: -9.0791 | Reg (weighted): 0.0658 | Total approx: -9.0133\n",
      "499/499 [==============================] - 47s 95ms/step - loss: -44720.0625 - val_loss: -46416.9023\n",
      "Epoch 479/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44936.4102\n",
      "Epoch 479: val_loss did not improve from -46960.91016\n",
      " Epoch 479 - NLL: -9.1239 | Reg (weighted): 0.0658 | Total approx: -9.0581\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -44936.4102 - val_loss: -46676.6055\n",
      "Epoch 480/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44922.7109\n",
      "Epoch 480: val_loss did not improve from -46960.91016\n",
      " Epoch 480 - NLL: -9.1179 | Reg (weighted): 0.0656 | Total approx: -9.0523\n",
      "499/499 [==============================] - 57s 115ms/step - loss: -44922.7109 - val_loss: -46572.2422\n",
      "Epoch 481/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44918.7383\n",
      "Epoch 481: val_loss did not improve from -46960.91016\n",
      " Epoch 481 - NLL: -9.1213 | Reg (weighted): 0.0659 | Total approx: -9.0555\n",
      "499/499 [==============================] - 53s 105ms/step - loss: -44918.7383 - val_loss: -46687.5312\n",
      "Epoch 482/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44939.2266\n",
      "Epoch 482: val_loss did not improve from -46960.91016\n",
      " Epoch 482 - NLL: -9.1295 | Reg (weighted): 0.0654 | Total approx: -9.0640\n",
      "499/499 [==============================] - 49s 99ms/step - loss: -44939.2266 - val_loss: -46842.8945\n",
      "Epoch 483/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44932.6953\n",
      "Epoch 483: val_loss did not improve from -46960.91016\n",
      " Epoch 483 - NLL: -9.0999 | Reg (weighted): 0.0660 | Total approx: -9.0340\n",
      "499/499 [==============================] - 48s 97ms/step - loss: -44932.6953 - val_loss: -45968.5820\n",
      "Epoch 484/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44944.8594\n",
      "Epoch 484: val_loss did not improve from -46960.91016\n",
      " Epoch 484 - NLL: -9.1227 | Reg (weighted): 0.0657 | Total approx: -9.0570\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44944.8594 - val_loss: -46602.9336\n",
      "Epoch 485/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44924.6680\n",
      "Epoch 485: val_loss did not improve from -46960.91016\n",
      " Epoch 485 - NLL: -9.1242 | Reg (weighted): 0.0655 | Total approx: -9.0588\n",
      "499/499 [==============================] - 52s 104ms/step - loss: -44924.6680 - val_loss: -46757.2852\n",
      "Epoch 486/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44895.8359\n",
      "Epoch 486: val_loss did not improve from -46960.91016\n",
      " Epoch 486 - NLL: -9.1167 | Reg (weighted): 0.0654 | Total approx: -9.0512\n",
      "499/499 [==============================] - 53s 107ms/step - loss: -44895.8359 - val_loss: -46675.9922\n",
      "Epoch 487/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44946.0781\n",
      "Epoch 487: val_loss did not improve from -46960.91016\n",
      " Epoch 487 - NLL: -9.1158 | Reg (weighted): 0.0658 | Total approx: -9.0500\n",
      "499/499 [==============================] - 54s 109ms/step - loss: -44946.0781 - val_loss: -46384.0273\n",
      "Epoch 488/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44924.3242\n",
      "Epoch 488: val_loss did not improve from -46960.91016\n",
      " Epoch 488 - NLL: -9.1128 | Reg (weighted): 0.0656 | Total approx: -9.0472\n",
      "499/499 [==============================] - 53s 106ms/step - loss: -44924.3242 - val_loss: -46408.6445\n",
      "Epoch 489/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44965.0195\n",
      "Epoch 489: val_loss did not improve from -46960.91016\n",
      " Epoch 489 - NLL: -9.1303 | Reg (weighted): 0.0657 | Total approx: -9.0646\n",
      "499/499 [==============================] - 48s 97ms/step - loss: -44965.0195 - val_loss: -46728.3516\n",
      "Epoch 490/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44847.1562\n",
      "Epoch 490: val_loss did not improve from -46960.91016\n",
      " Epoch 490 - NLL: -9.1092 | Reg (weighted): 0.0658 | Total approx: -9.0434\n",
      "499/499 [==============================] - 48s 96ms/step - loss: -44847.1562 - val_loss: -46684.9414\n",
      "Epoch 491/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44960.9805\n",
      "Epoch 491: val_loss did not improve from -46960.91016\n",
      " Epoch 491 - NLL: -9.1314 | Reg (weighted): 0.0656 | Total approx: -9.0658\n",
      "499/499 [==============================] - 56s 113ms/step - loss: -44960.9805 - val_loss: -46786.6641\n",
      "Epoch 492/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44964.5469\n",
      "Epoch 492: val_loss did not improve from -46960.91016\n",
      " Epoch 492 - NLL: -9.1236 | Reg (weighted): 0.0657 | Total approx: -9.0579\n",
      "499/499 [==============================] - 55s 111ms/step - loss: -44964.5469 - val_loss: -46529.9180\n",
      "Epoch 493/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44946.8477\n",
      "Epoch 493: val_loss did not improve from -46960.91016\n",
      " Epoch 493 - NLL: -9.1291 | Reg (weighted): 0.0654 | Total approx: -9.0637\n",
      "499/499 [==============================] - 56s 113ms/step - loss: -44946.8477 - val_loss: -46793.7461\n",
      "Epoch 494/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44958.1602\n",
      "Epoch 494: val_loss did not improve from -46960.91016\n",
      " Epoch 494 - NLL: -9.1287 | Reg (weighted): 0.0655 | Total approx: -9.0632\n",
      "499/499 [==============================] - 52s 105ms/step - loss: -44958.1602 - val_loss: -46721.8750\n",
      "Epoch 495/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44983.5625\n",
      "Epoch 495: val_loss did not improve from -46960.91016\n",
      " Epoch 495 - NLL: -9.1148 | Reg (weighted): 0.0661 | Total approx: -9.0487\n",
      "499/499 [==============================] - 49s 98ms/step - loss: -44983.5625 - val_loss: -46157.2461\n",
      "Epoch 496/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44937.5078\n",
      "Epoch 496: val_loss did not improve from -46960.91016\n",
      " Epoch 496 - NLL: -9.1086 | Reg (weighted): 0.0657 | Total approx: -9.0429\n",
      "499/499 [==============================] - 51s 102ms/step - loss: -44937.5078 - val_loss: -46214.4219\n",
      "Epoch 497/1000\n",
      "499/499 [==============================] - ETA: 0s - loss: -44963.8047\n",
      "Epoch 497: val_loss did not improve from -46960.91016\n",
      " Epoch 497 - NLL: -9.1142 | Reg (weighted): 0.0656 | Total approx: -9.0486\n",
      "499/499 [==============================] - 53s 105ms/step - loss: -44963.8047 - val_loss: -46254.3867\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        x=training_generator,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[es, mcp, csv_logger, print_epoch_loss],\n",
    "        epochs=1000,\n",
    "        shuffle=False,\n",
    "        steps_per_epoch=len(training_generator),\n",
    "        validation_steps=len(validation_generator),\n",
    "        verbose=1\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 kernel (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
